<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US" xml:lang="en-US">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Benjamin J. Heil" />
  <meta name="dcterms.date" content="2022-11-14" />
  <meta name="keywords" content="machine learning, science of science, reproducibility, citation network analysis" />
  <title>Dissertation Title</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <!--
  Manubot generated metadata rendered from header-includes-template.html.
  Suggest improvements at https://github.com/manubot/manubot/blob/main/manubot/process/header-includes-template.html
  -->
  <meta name="dc.format" content="text/html" />
  <meta name="dc.title" content="Dissertation Title" />
  <meta name="citation_title" content="Dissertation Title" />
  <meta property="og:title" content="Dissertation Title" />
  <meta property="twitter:title" content="Dissertation Title" />
  <meta name="dc.date" content="2022-11-14" />
  <meta name="citation_publication_date" content="2022-11-14" />
  <meta name="dc.language" content="en-US" />
  <meta name="citation_language" content="en-US" />
  <meta name="dc.relation.ispartof" content="Manubot" />
  <meta name="dc.publisher" content="Manubot" />
  <meta name="citation_journal_title" content="Manubot" />
  <meta name="citation_technical_report_institution" content="Manubot" />
  <meta name="citation_author" content="Benjamin J. Heil" />
  <meta name="citation_author_institution" content="Genomics and Computational Biology Graduate Group, University of Pennsylvania" />
  <meta name="citation_author_orcid" content="0000-0002-2811-1031" />
  <meta name="twitter:creator" content="@autobencoder" />
  <link rel="canonical" href="https://greenelab.github.io/ben_heil_dissertation/" />
  <meta property="og:url" content="https://greenelab.github.io/ben_heil_dissertation/" />
  <meta property="twitter:url" content="https://greenelab.github.io/ben_heil_dissertation/" />
  <meta name="citation_fulltext_html_url" content="https://greenelab.github.io/ben_heil_dissertation/" />
  <meta name="citation_pdf_url" content="https://greenelab.github.io/ben_heil_dissertation/manuscript.pdf" />
  <link rel="alternate" type="application/pdf" href="https://greenelab.github.io/ben_heil_dissertation/manuscript.pdf" />
  <link rel="alternate" type="text/html" href="https://greenelab.github.io/ben_heil_dissertation/v/3ddd800e44db097ce2421e5be4d096a1ae11e943/" />
  <meta name="manubot_html_url_versioned" content="https://greenelab.github.io/ben_heil_dissertation/v/3ddd800e44db097ce2421e5be4d096a1ae11e943/" />
  <meta name="manubot_pdf_url_versioned" content="https://greenelab.github.io/ben_heil_dissertation/v/3ddd800e44db097ce2421e5be4d096a1ae11e943/manuscript.pdf" />
  <meta property="og:type" content="article" />
  <meta property="twitter:card" content="summary_large_image" />
  <link rel="icon" type="image/png" sizes="192x192" href="https://manubot.org/favicon-192x192.png" />
  <link rel="mask-icon" href="https://manubot.org/safari-pinned-tab.svg" color="#ad1457" />
  <meta name="theme-color" content="#ad1457" />
  <!-- end Manubot generated metadata -->
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Dissertation Title</h1>
</header>
<p><small><em>
This manuscript
(<a href="https://greenelab.github.io/ben_heil_dissertation/v/3ddd800e44db097ce2421e5be4d096a1ae11e943/">permalink</a>)
was automatically generated
from <a href="https://github.com/greenelab/ben_heil_dissertation/tree/3ddd800e44db097ce2421e5be4d096a1ae11e943">greenelab/ben_heil_dissertation@3ddd800</a>
on November 14, 2022.
</em></small></p>
<h2 id="authors">Authors</h2>
<ul>
<li><strong>Benjamin J. Heil</strong>
<br>
<img src="images/orcid.svg" class="inline_icon" width="16" height="16" alt="ORCID icon" />
<a href="https://orcid.org/0000-0002-2811-1031">0000-0002-2811-1031</a>
· <img src="images/github.svg" class="inline_icon" width="16" height="16" alt="GitHub icon" />
<a href="https://github.com/benheil">benheil</a>
· <img src="images/twitter.svg" class="inline_icon" width="16" height="16" alt="Twitter icon" />
<a href="https://twitter.com/autobencoder">autobencoder</a><br>
<small>
Genomics and Computational Biology Graduate Group, University of Pennsylvania
</small></li>
</ul>
<div id="correspondence">
<p>✉ — Correspondence possible via <a href="https://github.com/greenelab/ben_heil_dissertation/issues">GitHub Issues</a></p>
</div>
<h2 class="page_break_before" id="abstract">Abstract</h2>
<h2 id="introduction">Introduction</h2>
<p>As computational biologists, we live in exciting times.
Beginning with the Human Genome Project <span class="citation" data-cites="14yhCmDBZ">[<a href="#ref-14yhCmDBZ" role="doc-biblioref">1</a>]</span> advancements in various technologies for biological quantification have generated data with a scale and granularity previously unimaginable <span class="citation" data-cites="11pOKbwng 7pjWPMNl 12pG2VA0G">[<a href="#ref-11pOKbwng" role="doc-biblioref">2</a>,<a href="#ref-7pjWPMNl" role="doc-biblioref">3</a>,<a href="#ref-12pG2VA0G" role="doc-biblioref">4</a>]</span>.</p>
<p>Concurrently with the skyrocketing amounts of data in computational biology, the advent of deep learning has generated methods designed specifically to make sense of large, complex datasets.
These methods have led to a paradigm shift in the field of machine learning, creating new possibilities in many fields and surfacing new phenomena unexplained by classical machine learning theory <span class="citation" data-cites="2gn6PKkv zJcT2HF sQO3gqho eZ5BSkaV">[<a href="#ref-2gn6PKkv" role="doc-biblioref">5</a>,<a href="#ref-zJcT2HF" role="doc-biblioref">6</a>,<a href="#ref-sQO3gqho" role="doc-biblioref">7</a>,<a href="#ref-eZ5BSkaV" role="doc-biblioref">8</a>]</span>.</p>
<p>The field of computational biology has long used traditional machine learning methods, as they help cope with the scale of the data being generated.
Accordingly, problem domains in computational biology that map well to existing research in deep learning have adopted or developed deep learning models and seen great advances <span class="citation" data-cites="TutLhFSz yZfcMIwh">[<a href="#ref-TutLhFSz" role="doc-biblioref">9</a>,<a href="#ref-yZfcMIwh" role="doc-biblioref">10</a>]</span>.</p>
<p>This dissertation explores the question of whether the paradigm shift in machine learning will spill over to computational biology.
That is to say have deep learning techniques fundamentally changed the field of computational biology, or are they (sometimes large) incremental improvements over existing methods?
Our thesis is that while deep learning provides us with useful tools for analyzing biological datasets, it doesn’t necessarily change the field on a fundamental level.</p>
<p>We begin with a few sections giving background information on previous research for the main thesis chapters.
We then move to chapter 2, which discusses standards necessary to ensure research done with deep learning is reproducible.
We continue to Chapter 3, where we find that deep learning models may not be helpful in analyzing expression data.
In chapters 4 and 5 we demonstrate that classical machine learning methods already allow scientists to uncover knowledge from large datasets.
Finally, in chapter 6 we conclude by discussing the implications of the previous chapters and their potential future directions.</p>
<h2 id="background-reproducibility">Background: Reproducibility</h2>
<h4 id="what-is-computational-reproducibility">What is computational reproducibility</h4>
<p>Reproducibility is a topic often discussed in scientific circles, and it means different things to different people <span class="citation" data-cites="D318Yc35 dwtrWar2 J1RPpvSe 1AET2xGSB kqrki6Ml 1BInrYmTW">[<a href="#ref-D318Yc35" role="doc-biblioref">11</a>,<a href="#ref-dwtrWar2" role="doc-biblioref">12</a>,<a href="#ref-J1RPpvSe" role="doc-biblioref">13</a>,<a href="#ref-1AET2xGSB" role="doc-biblioref">14</a>,<a href="#ref-kqrki6Ml" role="doc-biblioref">15</a>,<a href="#ref-1BInrYmTW" role="doc-biblioref">16</a>]</span>.
For the sake of clarity, we’ll operate from Stodden et al’s definition “the ability to recreate computational results from the data and code used by the original researcher” (http://stodden.net/icerm_report.pdf).
We would like to add one caveat though.
The language surrounding reproducibility is often binary, as in “reproducible research” or “an irreproducible paper”.
In reality, reproducibility falls on a sliding scale based on how long it takes a scientist to reproduce a work.
For poorly specified work, it could take forever as the conditions that allowed the research will never happen again.
For extremely high quality research, it could take a scientist only seconds of their time to press the “run” button on the original authors’ code and get the same results.</p>
<h4 id="why-does-it-matter">Why does it matter?</h4>
<p>Now that we’ve defined what reproducibility is, we can discuss why it matters.
In her book “Why Trust Science?”, Naomi Oreskes argues the answer to the eponymous question is that the process of coming to a consensus is what makes science trustworthy <span class="citation" data-cites="1oH3MHiP">[<a href="#ref-1oH3MHiP" role="doc-biblioref">17</a>]</span>.
In a world where all papers take forever to reproduce, it would be challenging to come to the consensus required to do trustworthy science.</p>
<p>Another way of viewing the scientific method is the Popperian idea of falsifiable theories <span class="citation" data-cites="g8wFsDHa">[<a href="#ref-g8wFsDHa" role="doc-biblioref">18</a>]</span>.
Theories are constructed from evidence and from reproduction of the same findings about the world.
If a theory can’t be reproduced, then it can’t be supported or proven false, and isn’t science under Popper’s definition <span class="citation" data-cites="Hopbpt36">[<a href="#ref-Hopbpt36" role="doc-biblioref">19</a>]</span>.</p>
<p>Those points are fairly philisophical though.
If you’re looking for a discussion of concrete impacts of failures in computational reproducibility, we recommend Ivie and Thain’s review paper <span class="citation" data-cites="JWFp2M7z">[<a href="#ref-JWFp2M7z" role="doc-biblioref">20</a>]</span>.
They point out that in the biological domain preclinical drug development studies could be replicated in only 10-25% of cases <span class="citation" data-cites="FsxIdYsZ yWY8AHZe">[<a href="#ref-FsxIdYsZ" role="doc-biblioref">21</a>,<a href="#ref-yWY8AHZe" role="doc-biblioref">22</a>]</span>.
Similarly, only about 50% of ACM papers were able to be built from their source code <span class="citation" data-cites="144brp8lV">[<a href="#ref-144brp8lV" role="doc-biblioref">23</a>]</span>.
In general, lack of reproducibility could cause a lack of trust in science <span class="citation" data-cites="80e7T2Rv">[<a href="#ref-80e7T2Rv" role="doc-biblioref">24</a>]</span>.</p>
<p>Reproducibility isn’t all about preventing bad things from happening though.
Having code that is easy to run helps verify that code is bug-free, and makes it easier for the original author to run in the future.
It also allows remixing research code, leading to greater accessibility of scientific research.
Because the authors working on latent diffusion models for image synthesis made their code available, others quickly created an optimized version allowing those without a powerful GPU to run it [<span class="citation" data-cites="zJcT2HF">[<a href="#ref-zJcT2HF" role="doc-biblioref">6</a>]</span>; https://github.com/CompVis/stable-diffusion; https://github.com/basujindal/stable-diffusion/]</p>
<h4 id="what-can-be-done">What can be done?</h4>
<p>The question remains: what can be done to increase the reproducibility of scientific work?
Krafczyk et al. argue that the keys are to document well, write code for ease of executability, and make code deterministic.
Alternatively, we could create a central repository of data and code used in research like we have a repository for articles in PubMed Central <span class="citation" data-cites="tPSSxxsX">[<a href="#ref-tPSSxxsX" role="doc-biblioref">25</a>]</span>.
In fact, the field of machine learning has something similar in Papers with Code (https://paperswithcode.com/), a website where you can browse only the machine learning preprints and papers that have associated code.
The epitome of reproducibility is probably something like executable papers a la Distill (https://distill.pub/) or eLife’s Executable Research Articles <span class="citation" data-cites="tsang2020">[<a href="#ref-tsang2020" role="doc-biblioref"><strong>tsang2020?</strong></a>]</span>.
In chapter 2, we discuss options to make machine learning research in the life sciences reproducible in more depth, and give our own recommendations.</p>
<h3 id="background-applications-of-machine-learning-in-transcriptomics">Background: Applications of machine learning in transcriptomics</h3>
<p>The human transcriptome provides a rich source of information about both healthy and disease states.
Not only is gene expression information useful for learning novel biological phenomena, it can also be used to diagnose and predict diseases.
These predictions have become more powerful in recent years as the field of machine learning has developed more methods.
In this section we review machine learning methods applied to predict various phenotypes from gene expression,
with a focus on the challenges in the field and what is being done to overcome them.
We close the review with potential areas for future research, as well as our perspectives on the strengths and weaknesses of supervised learning for phenotype prediction in particular.</p>
<p><strong>Introduction</strong><br />
Over the past few decades a number of tools for measuring gene expression have been developed.
As proteomics is currently difficult to do at a large scale, gene expression quantification methods are our best way to measure cells’ internal states.
While this wealth of information is promising, gene expression data is more difficult to work with than one might think.
The high dimensionality and instrument-driven variation require sophisticated techniques to separate the signal from the noise.</p>
<p>One such class of techniques is the set of methods from machine learning.
Machine learning methods depend on the assumption that there are patterns in data that can be learned to make predictions about future data.
Luckily, different people respond to the same disease in similar ways (for some diseases).
Learning genes that indicate an inflammatory response, for example, can help a machine learning model learn the difference between healthy and diseased expression samples.</p>
<p>There are many varieties of machine learning algorithms, so the scope of this paper is limited to analysis of supervised machine learning methods for phenotype prediction.
Supervised machine learning is a paradigm where the model attempts to predict labels.
For example, a model that predicts whether someone has lupus based on their gene expression data <span class="citation" data-cites="qs7B1fgV">[<a href="#ref-qs7B1fgV" role="doc-biblioref">26</a>]</span> is a supervised learning model.
In contrast, techniques for grouping data together without having phenotype labels are called unsupervised methods.
While these methods are also commonly used in computational biology <span class="citation" data-cites="19eIjTcxN 14mY2pXKf IRSA7yBm">[<a href="#ref-19eIjTcxN" role="doc-biblioref">27</a>,<a href="#ref-14mY2pXKf" role="doc-biblioref">28</a>,<a href="#ref-IRSA7yBm" role="doc-biblioref">29</a>]</span>, we will not be discussing them here.</p>
<p>The purpose of this review is to explain and analyze the various approaches that are used to predict phenotypes.
Each section of the review is centered around one of the challenges ubiquitous in using supervised machine learning techniques on gene expression data.
We hope to explain what has been tried and what the consensus for handling the challenge, if one exists.
The review will conclude with a section outlining promising new methods and areas where further study is needed.</p>
<p>If the field succeeds in addressing all the challenges, the payoffs will be substantial.
Being able to predict and diagnose diseases from whole blood gene expression is particularly interesting.
With sufficiently advanced analysis, invasive cancer biopsies might be able to be replaced with simple blood draws <span class="citation" data-cites="doi">[<a href="#ref-IRSA7yBm" role="doc-biblioref">29</a>]</span>.
If not, there are already diagnostics that predict various cancer aspects from biopsy gene expression <span class="citation" data-cites="7T8Uxprz">[<a href="#ref-7T8Uxprz" role="doc-biblioref">30</a>]</span>.
It may also be possible to diagnose common diseases based on blood gene expression <span class="citation" data-cites="Lphv6pyr dx78bdYB 1CNHzMFjN rJ5EpmYl">[<a href="#ref-Lphv6pyr" role="doc-biblioref">31</a>,<a href="#ref-dx78bdYB" role="doc-biblioref">32</a>,<a href="#ref-1CNHzMFjN" role="doc-biblioref">33</a>,<a href="#ref-rJ5EpmYl" role="doc-biblioref">34</a>]</span>, or even rare ones <span class="citation" data-cites="1ZboiGsF">[<a href="#ref-1ZboiGsF" role="doc-biblioref">35</a>]</span>.</p>
<p><strong>Background</strong><br />
The techniques for measuring gene expression and for analyzing it have changed dramatically over the past few decades.
This sections aims to explain what some of those changes are and how they affect phenotype prediction.</p>
<p><em>Gene expression</em><br />
Gene expression measurement methods have three main categories.
This first to be created is the gene expression microarray.
In a microarray, RNA is reverse transcribed to cDNA, labeled with fluorescent markers, then hybridized to probes corresponding to parts of genes.
The amount of fluorescence is then quantified to give the relative amount of gene expression for each gene.
While early microarrays had fewer genes and gene probes <span class="citation" data-cites="J9zGDTW4">[<a href="#ref-J9zGDTW4" role="doc-biblioref">36</a>]</span>, more modern ones measure tens of thousands of genes <span class="citation" data-cites="XeqEIk0K">[<a href="#ref-XeqEIk0K" role="doc-biblioref">37</a>]</span>.</p>
<p>While microarrays are useful, decreases in the price of genetic sequencing have made bulk RNA sequencing (RNA-seq) more common.
In RNA-seq, cDNA molecules are sequenced directly after being reverse transcribed from mRNA.
These cDNA fragments are then aligned against a reference exome to determine which gene, if any, each fragment maps to.
The output of the bulk RNA-seq pipeline is a list of genes and their corresponding read counts.
While there is not gene probe bias like in microarrays, RNA-seq has its own patterns of bias based on gene lengths and expression levels <span class="citation" data-cites="ChQys9ED">[<a href="#ref-ChQys9ED" role="doc-biblioref">38</a>]</span>.
Bulk RNA-seq is also unable to resolve heterogeneous populations of cells, as it measures the average gene expression of all of cells in the sample.</p>
<p>Fairly recently a new method was developed called single-cell RNA sequencing.
True to its name, single-cell sequencing allows gene expression to be measured at the individual cell level.
This increase in precision is accompanied by an increase in data sparsity though, as genes expressed infrequently or at low levels may not be detected.
The sparsity of single-cell data has led to a number of interesting methods, but as we worked with bulk RNA-sequencing single-cell papers will largely be absent from this review.</p>
<p><em>Machine Learning</em><br />
Machine learning has undergone a paradigm shift in the past decade, beginning with the publication of the AlexNet paper in 2012 <span class="citation" data-cites="fVSo2gZU">[<a href="#ref-fVSo2gZU" role="doc-biblioref">39</a>]</span>.
For decades random forests and support vector machines were the most widely used models in machine learning.
This changed dramatically when the AlexNet paper showed that neural networks could vastly outperform traditional methods in some domains <span class="citation" data-cites="fVSo2gZU">[<a href="#ref-fVSo2gZU" role="doc-biblioref">39</a>]</span>.
The deep learning revolution quickly followed, with deep neural networks becoming the state of the art in any problem with enough data <span class="citation" data-cites="c1tQ2yNq HpZqLKUe ld1EnZ6i yZfcMIwh">[<a href="#ref-yZfcMIwh" role="doc-biblioref">10</a>,<a href="#ref-c1tQ2yNq" role="doc-biblioref">40</a>,<a href="#ref-HpZqLKUe" role="doc-biblioref">41</a>,<a href="#ref-ld1EnZ6i" role="doc-biblioref">42</a>]</span>.</p>
<p>The implications of the deep learning revolution on this paper are twofold.
First, almost all papers before 2014 use traditional machine learning methods, while almost all papers after use deep learning methods.
Second, deep neural networks’ capacity to overfit the data and fail to generalize to outside data are vast.
We’ll show throughout the review various mistakes authors make because they don’t fully understand the failure states of neural networks and how to avoid them.</p>
<p><strong>Dimensionality Reduction</strong><br />
The most obvious challenge in working with gene expression data is its high dimensionality.
That is to say that the number of features (genes) in a dataset is typically greater than the number of samples.
It is common for an analysis to have tens of thousands of genes, but only hundreds (or tens) of samples.
Because even simple models struggle under such circumstances, it is necessary to find a representation of the data that uses fewer dimensions.</p>
<p>In the traditional machine learning paradigm, this is done via manual or heuristic feature selection methods.
Such methods tend to use a criterion like mutual information to select a subset of genes for the analysis <span class="citation" data-cites="MzuxpiPn">[<a href="#ref-MzuxpiPn" role="doc-biblioref">43</a>]</span>.
In one of the earliest papers in this review, Li et al. try a eight different methods from statistics and machine learning to see if any one in particular outperformed the others <span class="citation" data-cites="Vzg9ivZU">[<a href="#ref-Vzg9ivZU" role="doc-biblioref">44</a>]</span>.
Ultimately they found that no individual method rose to the top, and that the performance of different methods varies depending on the problem.</p>
<p>A number of other papers since then have also used manual methods.
Grewal et al. chose a subset of genes from COSMIC <span class="citation" data-cites="QSJEewCc">[<a href="#ref-QSJEewCc" role="doc-biblioref">45</a>]</span> for training, but found that their model performed better when using all genes instead of just a subset <span class="citation" data-cites="ClFEdqWg">[<a href="#ref-ClFEdqWg" role="doc-biblioref">46</a>]</span>.
Chen et al. used a different gene set.
They selected the LINCS 1000 gene set <span class="citation" data-cites="F7lIlh2N">[<a href="#ref-F7lIlh2N" role="doc-biblioref">47</a>]</span> for an imputation method, as the LINCS landmark genes are highly correlated with the genes they were trying to impute <span class="citation" data-cites="12QQw9p7v">[<a href="#ref-12QQw9p7v" role="doc-biblioref">48</a>]</span>.</p>
<p>Gene subsets can be based on prior knowledge of gene regulatory networks as well <span class="citation" data-cites="J0eeAld3 g8OoyIPj">[<a href="#ref-J0eeAld3" role="doc-biblioref">49</a>,<a href="#ref-g8OoyIPj" role="doc-biblioref">50</a>]</span>.
While very interpretable, these methods do not necessarily lead to increased performance in phenotype predictions <span class="citation" data-cites="IHi1L0uN">[<a href="#ref-IHi1L0uN" role="doc-biblioref">51</a>]</span>.
However, such methods can be useful in their own right.
PLIER (and the associated MultiPLIER framework) use prior knowledge genes to guide the latent variables learned by a matrix factorization technique <span class="citation" data-cites="Ki2ij7zE 14rnBunuZ">[<a href="#ref-Ki2ij7zE" role="doc-biblioref">52</a>,<a href="#ref-14rnBunuZ" role="doc-biblioref">53</a>]</span>.
The resulting latent variables can then be used in differential expression analyses in lieu of raw gene counts, allowing dimensionality reduction while guiding the learned variables towards biological relevance.</p>
<p>Selecting gene subsets via a heuristic or a machine learning model is also popular.
Sevakula et al. use decision stumps to select features then use a stacked autoencoder-type architecture to further compress the representation <span class="citation" data-cites="2i0SZtHv">[<a href="#ref-2i0SZtHv" role="doc-biblioref">54</a>]</span>.
Xiao et al. did something similar where they reduced the data to only genes were differentially expressed between their conditions of interest, then used a stacked autoencoder architecture <span class="citation" data-cites="uXRnjUvq">[<a href="#ref-uXRnjUvq" role="doc-biblioref">55</a>]</span>.
Instead of looking at raw differential expression, Dhruba et al. used another subsetting method called ReliefF <span class="citation" data-cites="bBLm1pxP">[<a href="#ref-bBLm1pxP" role="doc-biblioref">56</a>]</span> to find the top 200 genes for their source and target dataset, then kept the intersection for use in their model <span class="citation" data-cites="17srxvYZd">[<a href="#ref-17srxvYZd" role="doc-biblioref">57</a>]</span>.
More recently, Li et al. used a genetic algorithm for feature selection <span class="citation" data-cites="jxCyAK09">[<a href="#ref-jxCyAK09" role="doc-biblioref">58</a>]</span>.</p>
<p>Not all papers use a subset of the original genes in their analysis, however.
It is fairly common in recent years for authors to transform the data into a new lower dimensional space based on various metrics.
This used to be done via principle component analysis (PCA), a method that performs a linear transformation to
maximize the variance explained by a reduced number of dimensions <span class="citation" data-cites="CD9ApROI fakoor2013">[<a href="#ref-CD9ApROI" role="doc-biblioref">59</a>,<a href="#ref-fakoor2013" role="doc-biblioref"><strong>fakoor2013?</strong></a>]</span>.
Now scientists typically use different types of autoencoders, which learn a nonlinear mapping from the original space to a space with fewer dimensions.
Deepathology uses variational <span class="citation" data-cites="NLVTJ9Lj">[<a href="#ref-NLVTJ9Lj" role="doc-biblioref">60</a>]</span> and contractive <span class="citation" data-cites="14bFfy6t2">[<a href="#ref-14bFfy6t2" role="doc-biblioref">61</a>]</span> autoencoders in their model <span class="citation" data-cites="z3sjep1J">[<a href="#ref-z3sjep1J" role="doc-biblioref">62</a>]</span>, while Danaee et al. used a stacked denoising autoencoder <span class="citation" data-cites="vincent2010 11ZzdspWP">[<a href="#ref-11ZzdspWP" role="doc-biblioref">63</a>,<a href="#ref-vincent2010" role="doc-biblioref"><strong>vincent2010?</strong></a>]</span>.
Both papers compared their autoencoder dimensionality reduction to that of PCA and found that it performed better.
Danaee found that kernel PCA, a nonlinear version of PCA performed equivalently though.</p>
<p>It is also possible to use regularization methods to perform dimensionality reduction.
While they do not influence the nominal dimensionality of the data, they reduce the effective dimensionality by putting constraints on the input data or the model.
For example, SAUCIE uses an autoencoder structure, but combines it with a number of exotic regularization methods to further decrease the effective dimensionality of their data <span class="citation" data-cites="1CPSh19Mn">[<a href="#ref-1CPSh19Mn" role="doc-biblioref">64</a>]</span>.
In DeepType, Chen et al. use a more conventional elastic net regularization <span class="citation" data-cites="zou2005">[<a href="#ref-zou2005" role="doc-biblioref"><strong>zou2005?</strong></a>]</span> to induce sparsity in the first level of their network under the assumption that most genes’ expression will not affect a cancer’s subtype <span class="citation" data-cites="IRSA7yBm">[<a href="#ref-IRSA7yBm" role="doc-biblioref">29</a>]</span>.</p>
<p>Ultimately, there is no clear consensus in which dimensionality reduction methods perform the best.
Among the methods that transform the data there is a small amount of evidence that nonlinear transformations outperform linear ones, but only a few studies have tried both.
Going forward, a systematic evaluation of gene selection and dimensionality reduction methods on a variety of problems could be a huge asset to the
field.</p>
<p><strong>Batch Effects</strong><br />
When gene expression data comes from multiple studies, there are systematic differences between the samples even if they are measuring the same thing <span class="citation" data-cites="UPHmb9yc">[<a href="#ref-UPHmb9yc" role="doc-biblioref">65</a>]</span>.
These batch effects can bias the outcome of a study, and reduce the ability of a predictive model to generalize to data outside of the dataset used in the analysis.
Different studies handle this in different ways, with varying degrees of effectiveness.</p>
<p>Malta et al’s study is a good example of addressing batch effects well <span class="citation" data-cites="rYMEu6oz">[<a href="#ref-rYMEu6oz" role="doc-biblioref">66</a>]</span>.
They began by mean centering their data to ensure that the model didn’t learn to make classifications based on the mean gene expression values.
They then used Spearman correlation instead of Pearson correlation to avoid small changes in the data distributions to change their correlation measurement.
Finally they evaluated their results on a different data generation method (RNA-seq) from the one they trained on (microarray).</p>
<p>The SAUCIE paper handles batch effects very differently <span class="citation" data-cites="1CPSh19Mn">[<a href="#ref-1CPSh19Mn" role="doc-biblioref">64</a>]</span>.
They introduce a new type of regularization called maximal mean discrepancy, which penalizes the distance between the latent space representation between batches.
While this regularization term is deep learning specific and depends on the model having an embedding layer, it will be interesting to see if similar ideas are used in the future.</p>
<p>Other studies address batch effects less comprehensively via quantile normalization and 0-1 standardization <span class="citation" data-cites="12QQw9p7v fZGWNG2y">[<a href="#ref-12QQw9p7v" role="doc-biblioref">48</a>,<a href="#ref-fZGWNG2y" role="doc-biblioref">67</a>]</span>.
Using quantile normalization ensures that the different datasets have the same distribution, then 0-1 standardization makes machine learning algorithms treat all genes as equally important.
Another common technique from ML is to make decisions about the model based on cross-validation.
Since the feature or hyperparameter choice is validated on multiple random subsets of the data, batch effects are less likely to bias the decision <span class="citation" data-cites="jxCyAK09">[<a href="#ref-jxCyAK09" role="doc-biblioref">58</a> ]</span>.</p>
<p>Studies using contractive autoencoders <span class="citation" data-cites="z3sjep1J">[<a href="#ref-z3sjep1J" role="doc-biblioref">62</a>]</span> get some degree of batch effect protection just from their model constraints.
Since contractive autoencoders are trained to ignore small perturbations in the data, they tend to be more robust to distributional changes.
There are also more explicit ways of addressing batch effects.
DeepType, for example, uses the method ComBat <span class="citation" data-cites="1HahRBkyb">[<a href="#ref-1HahRBkyb" role="doc-biblioref">68</a>]</span> to reduce batch effects as a preprocessing step for their model <span class="citation" data-cites="IRSA7yBm">[<a href="#ref-IRSA7yBm" role="doc-biblioref">29</a>]</span>.</p>
<p>Unfortunately many studies don’t address batch effects at all, despite operating on large multi-study datasets like the Cancer Genome Atlas (TCGA).
These studies are likely to fail to generalize to real-world data, as machine learning models like to fixate on spurious correlations between data and phenotypes.</p>
<p><strong>Deep Learning vs Standard ML</strong>
As was discussed in the background section, recent years have seen a dramatic shift towards deep learning methods.
It is not immediately clear, however, whether this is a good decision for problems without giant datasets.
While some argue that deep learning is overrated and simpler models should be used instead <span class="citation" data-cites="EMrWUv3D GE6d2dOD">[<a href="#ref-EMrWUv3D" role="doc-biblioref">69</a>,<a href="#ref-GE6d2dOD" role="doc-biblioref">70</a>]</span>, others find that deep learning outperforms even domain specific models <span class="citation" data-cites="xSV2BrbO 15s31cve5">[<a href="#ref-xSV2BrbO" role="doc-biblioref">71</a>,<a href="#ref-15s31cve5" role="doc-biblioref">72</a>]</span>.</p>
<p>Because it is unclear which type of model will perform best on which dataset, it is important to try both simple and complex models.
In the Deepathology paper, Azarkahlili et al. found that their deep neural networks outperformed decision tree, KNN, random forest, logistic regression, and SVM models <span class="citation" data-cites="z3sjep1J">[<a href="#ref-z3sjep1J" role="doc-biblioref">62</a> ]</span>.
Likewise, in gene expression imputation, Chen et al. found that their neural network classifier outperformed linear regression in 99.97 percent of genes and k-nearest neighbors in all genes <span class="citation" data-cites="12QQw9p7v">[<a href="#ref-12QQw9p7v" role="doc-biblioref">48</a>]</span>.
On the other hand, Grewal et al. tried multiple methods and found they work roughly the same <span class="citation" data-cites="ClFEdqWg">[<a href="#ref-ClFEdqWg" role="doc-biblioref">46</a>]</span>.
They settled this by combining a few different models into an ensemble.</p>
<p>Due to technical considerations <span class="citation" data-cites="rYMEu6oz">[<a href="#ref-rYMEu6oz" role="doc-biblioref">66</a>]</span> or other reasons, some authors only evaluate a single model <span class="citation" data-cites="uXRnjUvq">[<a href="#ref-uXRnjUvq" role="doc-biblioref">55</a>]</span>.
While this simplifies the analysis for their papers, it makes it unclear whether they could have done better with a different model.
This is particularly important for authors who are using deep learning models, because simpler models tend to be much more interpretable.</p>
<p><strong>Evaluating Model Performance</strong><br />
Validation is another important consideration in phenotype prediction.
The gold standard of validation would be a knockout and rescue assay demonstrating that the predicted mechanism or expression relationship truly exists.
Since machine learning models make predictions of nonlinear relationships between thousands of genes, however, such validation isn’t feasible.
Instead scientists evaluate their models’ efficacy by testing their performance on data they didn’t train them on.
Test datasets can be built in different ways, assorting roughly into three tiers based on their external validity.</p>
<p>The most basic method is referred to as cross-validation.
In cross-validation, the training data is split into a training and validation dataset.
The model is trained on the training dataset, then its performance is measured on the validation dataset.
Typically this is done with a process called five-fold cross-validation, where the process is repeated five times on five different ways of splitting up the training data.
This method is common <span class="citation" data-cites="17srxvYZd jxCyAK09 11ZzdspWP">[<a href="#ref-17srxvYZd" role="doc-biblioref">57</a>,<a href="#ref-jxCyAK09" role="doc-biblioref">58</a>,<a href="#ref-11ZzdspWP" role="doc-biblioref">63</a>]</span>, but isn’t really a rigorous evaluation.
Because the same dataset is used for both selecting a model and measuring performance, the data can ’go stale’ when you test several models <span class="citation" data-cites="KE9BUIOX">[<a href="#ref-KE9BUIOX" role="doc-biblioref">73</a>]</span>.
In the extreme case, it is possible to get 100% accuracy by testing random prediction schemes on the data.</p>
<p>In order to keep data fresh, some researchers use a more rigorous method called a held out test set<span class="citation" data-cites="z3sjep1J 2i0SZtHv">[<a href="#ref-2i0SZtHv" role="doc-biblioref">54</a>,<a href="#ref-z3sjep1J" role="doc-biblioref">62</a>]</span>.
In the held out test set paradigm, a portion of the dataset is set aside and effectively put in a locked box until the end of the analysis.
Once the model architecture, hyperparameters, and dimensionality reduction decisions are all made via cross-validation on the training data, the lock box can be opened and the data within used for evaluation.
As the lock box data is only used once, it has no risk of becoming stale due to multiple testing.
The only drawback to this method is that is depends on the assumption that the data in the real world is distributed the same as the data in your training set.</p>
<p>The best (and most difficult) way to evaluate a model is by using an independent dataset.
Ideally, an independent dataset is created by a different group or on a different expression quantification platform.
For example, once their model was trained, Chen et al. evaluated their model on a dataset from GEO, a dataset from GTEx, and a cancer cell line <span class="citation" data-cites="12QQw9p7v">[<a href="#ref-12QQw9p7v" role="doc-biblioref">48</a>]</span>.
It is also possible to use combinations of validation methods.
In their paper Grewal et al. used a held-out section of their original data, then went on to evaluate their model in an independent dataset <span class="citation" data-cites="ClFEdqWg">[<a href="#ref-ClFEdqWg" role="doc-biblioref">46</a>]</span>.
Similarly, Malta et al. used cross-validation initially, but then evaluated their model on an external microarray dataset to ensure their data wasn’t stale <span class="citation" data-cites="rYMEu6oz">[<a href="#ref-rYMEu6oz" role="doc-biblioref">66</a>]</span>.
Likewise, Deng at al. initially benchmark their model on various simulated data sets, but then go on to validate their model on real data <span class="citation" data-cites="V3nGUaio">[<a href="#ref-V3nGUaio" role="doc-biblioref">74</a>]</span>.</p>
<p>Ultimately researchers work with what they have, and it’s not always possible to acquire an independent dataset.
That being said, it is always worth keeping the different tiers of external validity in mind when evaluating papers that use machine learning.</p>
<p><strong>Transfer Learning</strong><br />
Transfer learning is a field of machine learning that uses information from outside of the training dataset to improve model performance.
Techniques from the field of transfer learning are particularly useful in the domain of gene expression, because there are large databases like GEO and TCGA that contain data that may be useful in prediction tasks.
In this section we’ll focus in on two types of transfer learning that are particularly useful: multitask learning and semi-supervised learning.</p>
<p>Multitask learning involves training a model on multiple problems in order to improve the model’s performance on a problem of interest.
As gene expression patterns can be shared across diseases <span class="citation" data-cites="Uot2y2ws 7k4Mlul7">[<a href="#ref-Uot2y2ws" role="doc-biblioref">75</a>,<a href="#ref-7k4Mlul7" role="doc-biblioref">76</a>]</span>, the extra data can help increase the model’s power.
For example, instead of training a model to learn one drug response at a time, Yuan et al.
had better results predicting all the drugs in their dataset simultaneously <span class="citation" data-cites="URjcKCcA">[<a href="#ref-URjcKCcA" role="doc-biblioref">77</a>]</span>.
Similarly, Deepathology predicts tissue type, disease, and miRNA expression simultaneously <span class="citation" data-cites="z3sjep1J">[<a href="#ref-z3sjep1J" role="doc-biblioref">62</a>]</span>.
It is worth noting that multitask learning works best when using a deep learning model.
When using standard machine learning it is necessary to perform some difficult data transformation to do classification on multiple classes <span class="citation" data-cites="Vzg9ivZU">[<a href="#ref-Vzg9ivZU" role="doc-biblioref">44</a>]</span>.</p>
<p>Where supervised learning uses entirely labeled data, semi-supervised learning takes advantage of unlabeled data as well.
The most popular way of doing semi-supervised learning is to use an autoencoder structure to initialize your model’s weights.
Where most models begin training with a randomly initialized set of weights, it is possible to initially train a neural network to create a compressed representation of the input data (an encoding).
The weights that it learns in the process often turn out to be a better initialization when the labeled training data is finally brought in.
There are a number of ways to perform the autoencoding step.
Instead of training all the layers of the network simultaneously, it is possible to train one layer to create the encoding at a time <span class="citation" data-cites="2i0SZtHv uXRnjUvq">[<a href="#ref-2i0SZtHv" role="doc-biblioref">54</a>,<a href="#ref-uXRnjUvq" role="doc-biblioref">55</a>]</span>.
This is referred to as a stacked autoencoder.
One can also train the whole network at the same time, as Danaee et al do with their denoising autoencoder <span class="citation" data-cites="11ZzdspWP">[<a href="#ref-11ZzdspWP" role="doc-biblioref">63</a>]</span>.
Not all methods are autoencoder-based though.
Dhruba et al. develop their own semi-supervised learning process that teaches a model to learn a latent space between classes <span class="citation" data-cites="17srxvYZd">[<a href="#ref-17srxvYZd" role="doc-biblioref">57</a>]</span>.</p>
<p><strong>Future Directions</strong><br />
Upon reviewing a broad spectrum of what has been done in the field, a few opportunities for future research have become clear.</p>
<p>As shown in the batch effects section, authors handle batch effects in their studies with varying degrees of sophistication.
The studies we have discussed use various strategies to mitigate the technical variation between studies and batches, but it may be possible to do better.
Recent developments in the field of transfer learning have lead to methods that use technical variation between samples to increase the power of an analysis <span class="citation" data-cites="1DeCFT6PA XR77uqve">[<a href="#ref-1DeCFT6PA" role="doc-biblioref">78</a>,<a href="#ref-XR77uqve" role="doc-biblioref">79</a>]</span>.
These methods exist on the bleeding edge of transfer learning, but gene expression data fits their assumptions very well.
It would be interesting to see if models trained with such methods would be more successful than those using traditional batch effect correction.</p>
<p>While many models have been used to make predictions from gene expression, it’s unclear which ones work best, and in which circumstances.
One review evaluated a variety of unsupervised methods on gene regulatory network discovery, but the only supervised method that was tried was a support vector machine <span class="citation" data-cites="mh4sXNBi">[<a href="#ref-mh4sXNBi" role="doc-biblioref">80</a>]</span>.
A large scale study comparing methods to each other would be very useful to the field.
Of particular interest would be a study that determines roughly how many samples are needed before it deep learning models outperform traditional machine learning models, and how semi-supervised learning shifts that change point.</p>
<p>Semi-supervised learning is a technique that began being applied to gene expression data only recently.
While the technique has been useful when applied to large amounts of unlabeled data, the effects of which unlabeled dataset(s) are used hasn’t been measured.
Due to the large differences between RNA-seq and microarray data, it may make sense to do pretraining with just GEO or Recount3 <span class="citation" data-cites="xdJRSgce">[<a href="#ref-xdJRSgce" role="doc-biblioref">81</a>]</span> depending on whether the labeled data is primarily from microarrays or RNA-seq.
A study looking at whether more data is always better, and whether using data from a different platform helps or hurts would be a useful reference for those using semi-supervised learning to train their models.</p>
<p>Looking closer at how to do multitask learning could also help the field.
While several studies in this review have analyzed multitask learning, there is not a study that we know of that determines exactly how similar the classes should be for gene expression data.
Testing various methods from Sebastian Ruder’s multitask learning review paper could help find a heuristic for how similar phenotypes should be in multitask learning <span class="citation" data-cites="DKDxhmok">[<a href="#ref-DKDxhmok" role="doc-biblioref">82</a>]</span>.</p>
<p>For the most part the studies in this review either learn how to diagnose a specific phenotype with a small dataset, or learn more classes by studying TCGA data.
We believe that there is an opportunity for datasets to be created from Refine.bio (https://www.refine.bio/) and Recount3 <span class="citation" data-cites="xdJRSgce">[<a href="#ref-xdJRSgce" role="doc-biblioref">81</a>]</span> data that would be able to predict phenotypes other than just cancer on a large dataset.
The consistent preprocessing for these resources makes their gene expression data much easier to use with machine learning methods.</p>
<p><strong>Conclusion and Perspectives</strong><br />
Making predictions from gene expression information holds great promise, and is already being used in some cases.
Because the problem space lies between the fields of machine learning and computational biology, however, it inherits pitfalls from both fields.
Frequently, biologists who want to attempt to make models will fail to understand how to do model validation and hyperparameter tuning in a way that doesn’t invalidate their results.
Likewise, machine learning researchers often will leak information between the training and the testing set by blindly randomizing all their samples, or will fail to account for the batch effects inherent to muli-study datasets.</p>
<p>In addition to the challenges from working across disciplines, the approaches used in making predictions are largely fragmented.
Researchers make decisions about their model architecture, dimensionality reduction, and batch effect correction largely based on their intuition.
There have been few papers evaluating methods across several problems, and even less consensus about which methods work the best.
Moving forward, the field will need to consolidate and determine a set of best practices to reduce the model search space for new papers.
Likewise, researchers will need to begin working with clinicians and wet-lab scientists to validate whether their models work in vivo as well as in-silico.
Ultimately, phenotype predictions from gene expression appear to have have a bright future.
In order to get there, however, there are many challenges that need to be addressed.</p>
<h3 id="reproducible-research-background">Reproducible research background</h3>
<h3 id="citation-indices-background">Citation indices background</h3>
<h3 id="talk-briefly-about-conclusion-chapter-probably-write-this-part-once-its-done">Talk briefly about conclusion chapter (probably write this part once it’s done)</h3>
<h2 id="citation-index-review">Citation index review</h2>
<p>Over the past century quantifying the progress of science has become popular.
Even before computers made it easy to collate information about publications, work had already begun to evaluate papers based on their number of citations <span class="citation" data-cites="dmxNoeYV">[<a href="#ref-dmxNoeYV" role="doc-biblioref">83</a>]</span>.
There’s even a book about it <span class="citation" data-cites="107JaW2qS">[<a href="#ref-107JaW2qS" role="doc-biblioref">84</a>]</span>.</p>
<p>Determining the relative “impact” of different authors and journals is a perennial question when measuring science.
One of the most commonly used metric in this space is the h-index, which balances an authors quantity of publications with the number of citations each one receives <span class="citation" data-cites="qXsETMbA">[<a href="#ref-qXsETMbA" role="doc-biblioref">85</a>]</span>.
However, the h-index is not a perfect metric <span class="citation" data-cites="1bkadVmO">[<a href="#ref-1bkadVmO" role="doc-biblioref">86</a>]</span>, and has arguably become less useful in recent years <span class="citation" data-cites="OsrIUdWt">[<a href="#ref-OsrIUdWt" role="doc-biblioref">87</a>]</span>.
Other metrics, like the g-index<span class="citation" data-cites="iDjOlq4q">[<a href="#ref-iDjOlq4q" role="doc-biblioref">88</a>]</span> and the i-10 index (https://scholar.google.com/), try to improve on the h-index by placing a higher weight on more highly cited papers.</p>
<p>There are metrics for comparing journals as well.
The Journal Impact Factor <span class="citation" data-cites="16V0td5f2">[<a href="#ref-16V0td5f2" role="doc-biblioref">89</a>]</span> is the progenitor journal metric, evaluating journals based on how many citations the average paper in that journal has received over the past few years.
Other measures use a more network-based approach to quantifying journals’ importance.
The most common are Eigenfactor <span class="citation" data-cites="ENhL9A6A">[<a href="#ref-ENhL9A6A" role="doc-biblioref">90</a>]</span> and the SCImago Journal Rank (https://www.scimagojr.com/), which use variations on the PageRank algorithm to evaluate the importance of various journals.</p>
<p>Academic articles are arguably the main building blocks of scientific communication, so it makes sense to try to better understand which ones are the most important.
Citation count seems like an obvious choice, but differences in citation practices between fields <span class="citation" data-cites="5vaZVhmk">[<a href="#ref-5vaZVhmk" role="doc-biblioref">91</a>]</span> make it too crude a measure of impact.
Instead, many other metrics have been developed to choose which papers to read.</p>
<p>Many of these methods work by analyzing the graph formed by treating articles as nodes and citations as edges.
PageRank<span class="citation" data-cites="zeuTsDVX">[<a href="#ref-zeuTsDVX" role="doc-biblioref">92</a>]</span>, one of the most influential methods for ranking nodes’ importance in a graph, can also be applied to ranking papers <span class="citation" data-cites="ZiPXgNU5">[<a href="#ref-ZiPXgNU5" role="doc-biblioref">93</a>]</span>.
It isn’t the only graph-based method though.
Other methods of centrality such as betweenness centrality would make sense to use, but are prohibitively computationally expensive to run.
Instead, methods like the disruption index <span class="citation" data-cites="UTLLCTKy">[<a href="#ref-UTLLCTKy" role="doc-biblioref">94</a>]</span> and its variants <span class="citation" data-cites="PGRGcmLi">[<a href="#ref-PGRGcmLi" role="doc-biblioref">95</a>]</span> are more often used.</p>
<p>Some lines of research try to quanitfy other desirable characteristic of papers.
For example, Foster et al. claim to measure innovation by looking at papers that create new connections between known chemical entites <span class="citation" data-cites="qT77Z7V">[<a href="#ref-qT77Z7V" role="doc-biblioref">96</a>]</span>.
Likewise, Wang et al. define novel papers as those that cite papers from unusual combinations of journals <span class="citation" data-cites="RzTov9Er">[<a href="#ref-RzTov9Er" role="doc-biblioref">97</a>]</span>.
The Altmetric Attention Score (https://www.altmetric.com/) goes even further, measuring the attention on a paper from outside the standard academic channels.</p>
<p>These metrics don’t stand alone, however.
Lots of work has gone into improving the various methods by shoring up their weaknesses or normalizing them to make them more comparable across fields.
The relative citation ratio makes citation counts comparable across fields by normalizing it according to other papers in its neighborhood of the citation network <span class="citation" data-cites="kZI40ZXS">[<a href="#ref-kZI40ZXS" role="doc-biblioref">98</a>]</span>.
Similarly, the source-normalized impact per paper normalizes the citation count for based on the total number of citations in the whole field <span class="citation" data-cites="11IkPxdIL">[<a href="#ref-11IkPxdIL" role="doc-biblioref">99</a>]</span>.
Several methods modify PageRank, such as Topical PageRank, which incorporates topic and journal prestige information to the PageRank calculation <span class="citation" data-cites="12CYfIcWF">[<a href="#ref-12CYfIcWF" role="doc-biblioref">100</a>]</span>, and
Vaccario et al’s page and field rescaled PageRank which accounts for differences between papers’ ages and fields <span class="citation" data-cites="4F5AdeXW">[<a href="#ref-4F5AdeXW" role="doc-biblioref">101</a>]</span>.
There are also a number of variants of the disruption index <span class="citation" data-cites="PGRGcmLi">[<a href="#ref-PGRGcmLi" role="doc-biblioref">95</a>]</span>.</p>
<p>Of course, none of these methods would be possible without data to train and evaluate them on.
We’ve come a long way from Garfield’s “not unreasonable” proposal to manually aggregate one million citations <span class="citation" data-cites="dmxNoeYV">[<a href="#ref-dmxNoeYV" role="doc-biblioref">83</a>]</span>.
These days we have several datasets with hundreds of millions to billions of references (https://www.webofknowledge.com, https://www.scopus.com <span class="citation" data-cites="126JIm8me">[<a href="#ref-126JIm8me" role="doc-biblioref">102</a>]</span>).</p>
<p>Quantifying science isn’t perfect, however.
In addition to shortcomings of individual methods <span class="citation" data-cites="1CjuavV0Y DupOCXrp Qd54lMyE">[<a href="#ref-1CjuavV0Y" role="doc-biblioref">103</a>,<a href="#ref-DupOCXrp" role="doc-biblioref">104</a>,<a href="#ref-Qd54lMyE" role="doc-biblioref">105</a>]</span>, there are issues inherent to reducing the process of science to numbers.
To quote Alfred Korzybski, “the map is not the territory.”
Metrics of science truly measure quantitative relationships like mean citation counts, despite purporting to reflect “impact”, “disruption”, or “novelty”.
If we forget that, we can mistake useful tools for arbiters of ground truth.</p>
<p>In chapter <strong>X</strong>, we dive into one such shortcoming, by demonstrating differences in article PageRanks between fields.
There we argue that normalizing out field-specific differences obscures useful signal, and propose new directions of research for future citation metrics.</p>
<h2 id="future-directions">Future directions</h2>
<p>In this dissertation, we have examined whether deep learning has led to a paradigm shift in computational biology.
We established standards for reproducible research when using deep learning models in chapter 2, showed that deep learning isn’t always preferable to other techniques in chapter 3, then demonstrated the effectiveness of classical ml methods in chapters 4 and 5.
Ultimately we came to the conclusion that while deep learning has been a useful tool in some areas, it ultimately has not led to a paradigm shift in computational biology.
However, deep learning models’ impact may grow as the fields develop, so we’d like to discuss future areas where we expect interesting developments to happen.</p>
<h3 id="deep-learning-representations-of-biology">Deep learning representations of biology</h3>
<p>Different areas of computational biology research have seen different effects from deep learning.
Deep learning has already had a large impact on biomedical imaging <span class="citation" data-cites="o78W9pGh">[<a href="#ref-o78W9pGh" role="doc-biblioref">106</a>]</span>, and seems poised to do so in protein structure <span class="citation" data-cites="yZfcMIwh">[<a href="#ref-yZfcMIwh" role="doc-biblioref">10</a>]</span>.
These advances were likely successful because of their similarity to well-researched fields in that they can be framed as similar problems.
Biomedical images aren’t exactly the same as those from a normal camera, but the inductive bias of translational equivariance and various image augmentation methods are still applicable.
Similarly, while protein sequences may not seem to share much with written language, models like RNNs and transformers that look at input as a sequence of tokens don’t care whether those tokens are words or amino acids.</p>
<p>Not all subfields of computational biology have convenient ways to represent their data though.
Gene expression in particular is difficult because of its high dimensionality.
Expression data doesn’t have spatial locality to take advantage of, so convolutional networks can’t be used to ignore it.
It’s not a series of tokens either; the genes in an expression dataset are listed lexicographically so their order doesn’t have meaning.
Self-attention seems well-suited for for gene expression since learning which subsets of genes interact with others would be useful.
The high dimensionality makes vanilla self-attention infeasible though, due to the quadratic scaling.
You can’t even sidestep the issue with standard dimensionality reduction methods without losing predictive performance.</p>
<p>Do any deep learning representations work for gene expression then?
Fully-connected networks work, though they don’t tend to be the best way to accomplish most tasks.
An interesting potential direction of research would be to apply sparse self-attention methods to gene expression data and reduce the number of comparisons made by only attending within prior knowledge gene sets.
Alternatively, because expression is often thought of in terms of coregulation networks or sets of genes with shared functions a graph representation may be more suitable.
It’s also possible that someone will develop a representation specifically for gene expression that will work better than anything we know about today.</p>
<h3 id="to-what-extent-is-biology-limited-by-challenges-in-looking-at-the-data">To what extent is biology limited by challenges in looking at the data</h3>
<p>An important first step when working with data is to look at it.
In images of generated text, a human can make a judgement on how good generated data is.
In the classification world, a human labeler can look at an image and say “that is a dog” or a sentence and say “that is grammatically correct english.”
While these labels are somewhat fuzzy, a group of humans can at least look at the label and say “that is reasonable” or “that is mislabeled.”
A human looking at a gene expression microarray, or a table of RNA-seq counts is unable to do the same.</p>
<p>Our brains are built to recognize objects, not parse gene expression perturbations corresponding to septic shock.
This issue isn’t insurmountable; scientists are able to do research in quantum physics after all.
It simply serves as hinderence on our ability to sanity check data.
Because we can’t see whether the relevant signals are distorted by batch effect normalization or a preprocessing step.
Perhaps in the future, as we understand more about the relevant biology, scientists will be able to create views of the data that are more human-intuitive and easier to use.</p>
<h3 id="the-scale-of-biological-data">The scale of biological data</h3>
<p>Biological data (or at least transcriptomic data) isn’t actually that big.
The largest uniformly processed compendia of bulk human expression data are on the order of hundreds of thousands of samples.
Meanwhile in machine learning, even before deep learning took off ImageNet already have more than three million images <span class="citation" data-cites="lt4BNUoG">[<a href="#ref-lt4BNUoG" role="doc-biblioref">107</a>]</span>.</p>
<p>Worse, many biological domains have strict upper bounds on the amount of data available.
Even if one somehow recruited the entire world for a study they would only be able to collect around eight billion human genomes.
Given the complexity of biology it seems unlikely that “only” eight billion geneomes would be sufficient to effectively sample the space of plausible relevant mutations in the human genome.
Based on recent research into neural network scaling laws <span class="citation" data-cites="XVMFUrSt">[<a href="#ref-XVMFUrSt" role="doc-biblioref">108</a>]</span> and machine learning intuition, it seems likely that Rich Sutton’s “Bitter Lesson” (http://www.incompleteideas.net/IncIdeas/BitterLesson.html) would break down in a domain where there is a hard cap on the available data.
This data cap probably isn’t true of all domains in computational biology though.
Gene expression changes with variables like cell type, time, and biological state, so the space of transcriptomic data that could be measured is much larger.</p>
<p>While we’ve shown that deep learning hasn’t lead to a paradigm shift in computational biology so far, will that always be true?
As with many scientific questions, the answer is probably “it depends”.
While there may be caps on individual aspects of biological data, there are always more angles of attack.</p>
<p>The promise of multiomics has always been that multiple views of the same system may reveal something that no single view picks up.
The challenge is that the data types are different, their relationships are not well-characterized, and the methods for working in such a system haven’t been fully developed yet.
Transformer architectures, and more specifically their self-attention mechanism seem like a good fit for learning relationships between different ’omes.
Such models are data hungry though, and self-attention gets expensive in problems with high dimensionality.
Perhaps one day we’ll have the data and compute to train multiomic biological transformers.
Or maybe by that point the state of the art in machine learning will have moved along rendering that point moot.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Regardless of whether deep learning takes over or just becomes another tool in our toolbelt, the future of computational biology looks bright.
These are exciting times indeed.</p>
<h2 class="page_break_before" id="references">References</h2>
<!-- Explicitly insert bibliography here -->
<div id="refs" class="references csl-bib-body" role="doc-bibliography">
<div id="ref-14yhCmDBZ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">1. </div><div class="csl-right-inline"><strong>Initial sequencing and analysis of the human genome</strong><div class="csl-block">, Eric S Lander, Lauren M Linton, Bruce Birren, Chad Nusbaum, Michael C Zody, Jennifer Baldwin, Keri Devon, Ken Dewar, Michael Doyle, … </div> <em>Nature</em> (2001-02-15) <a href="https://doi.org/bfpgjh">https://doi.org/bfpgjh</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/35057062">10.1038/35057062</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/11237011">11237011</a></div></div>
</div>
<div id="ref-11pOKbwng" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">2. </div><div class="csl-right-inline"><strong>Highly Parallel Genome-wide Expression Profiling of Individual Cells Using Nanoliter Droplets</strong> <div class="csl-block">Evan Z Macosko, Anindita Basu, Rahul Satija, James Nemesh, Karthik Shekhar, Melissa Goldman, Itay Tirosh, Allison R Bialas, Nolan Kamitaki, Emily M Martersteck, … Steven A McCarroll</div> <em>Cell</em> (2015-05) <a href="https://doi.org/f7dkxv">https://doi.org/f7dkxv</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.cell.2015.05.002">10.1016/j.cell.2015.05.002</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26000488">26000488</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4481139">PMC4481139</a></div></div>
</div>
<div id="ref-7pjWPMNl" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">3. </div><div class="csl-right-inline"><strong>An efficient targeted nuclease strategy for high-resolution mapping of DNA binding sites</strong> <div class="csl-block">Peter J Skene, Steven Henikoff</div> <em>eLife</em> (2017-01-16) <a href="https://doi.org/gfkh8w">https://doi.org/gfkh8w</a> <div class="csl-block">DOI: <a href="https://doi.org/10.7554/elife.21856">10.7554/elife.21856</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28079019">28079019</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5310842">PMC5310842</a></div></div>
</div>
<div id="ref-12pG2VA0G" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">4. </div><div class="csl-right-inline"><strong>The second decade of 3C technologies: detailed insights into nuclear organization</strong> <div class="csl-block">Annette Denker, Wouter de Laat</div> <em>Genes &amp;amp; Development</em> (2016-06-15) <a href="https://doi.org/gdcfmg">https://doi.org/gdcfmg</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/gad.281964.116">10.1101/gad.281964.116</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27340173">27340173</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4926860">PMC4926860</a></div></div>
</div>
<div id="ref-2gn6PKkv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">5. </div><div class="csl-right-inline"><strong>Mastering the game of Go with deep neural networks and tree search</strong> <div class="csl-block">David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, … Demis Hassabis</div> <em>Nature</em> (2016-01-27) <a href="https://doi.org/f77tw6">https://doi.org/f77tw6</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nature16961">10.1038/nature16961</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26819042">26819042</a></div></div>
</div>
<div id="ref-zJcT2HF" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">6. </div><div class="csl-right-inline"><strong>High-Resolution Image Synthesis with Latent Diffusion Models</strong> <div class="csl-block">Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer</div> <em>arXiv</em> (2022-04-14) <a href="https://arxiv.org/abs/2112.10752">https://arxiv.org/abs/2112.10752</a></div>
</div>
<div id="ref-sQO3gqho" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">7. </div><div class="csl-right-inline"><strong>Deep Double Descent: Where Bigger Models and More Data Hurt</strong> <div class="csl-block">Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, Ilya Sutskever</div> <em>arXiv</em> (2019-12-06) <a href="https://arxiv.org/abs/1912.02292">https://arxiv.org/abs/1912.02292</a></div>
</div>
<div id="ref-eZ5BSkaV" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">8. </div><div class="csl-right-inline"><strong>Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets</strong> <div class="csl-block">Alethea Power, Yuri Burda, Harri Edwards, Igor Babuschkin, Vedant Misra</div> <em>arXiv</em> (2022-01-07) <a href="https://arxiv.org/abs/2201.02177">https://arxiv.org/abs/2201.02177</a></div>
</div>
<div id="ref-TutLhFSz" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">9. </div><div class="csl-right-inline"><strong>U-Net: Convolutional Networks for Biomedical Image Segmentation</strong> <div class="csl-block">Olaf Ronneberger, Philipp Fischer, Thomas Brox</div> <em>Lecture Notes in Computer Science</em> (2015) <a href="https://doi.org/gcgk7j">https://doi.org/gcgk7j</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/978-3-319-24574-4_28">10.1007/978-3-319-24574-4_28</a></div></div>
</div>
<div id="ref-yZfcMIwh" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">10. </div><div class="csl-right-inline"><strong>Highly accurate protein structure prediction with AlphaFold</strong> <div class="csl-block">John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin Žídek, Anna Potapenko, … Demis Hassabis</div> <em>Nature</em> (2021-07-15) <a href="https://doi.org/gk7nfp">https://doi.org/gk7nfp</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41586-021-03819-2">10.1038/s41586-021-03819-2</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/34265844">34265844</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8371605">PMC8371605</a></div></div>
</div>
<div id="ref-D318Yc35" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">11. </div><div class="csl-right-inline"><strong>Electronic documents give reproducible research a new meaning</strong> <div class="csl-block">Jon F Claerbout, Martin Karrenbach</div> <em>SEG Technical Program Expanded Abstracts 1992</em> (1992-01) <a href="https://doi.org/b6t7wj">https://doi.org/b6t7wj</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1190/1.1822162">10.1190/1.1822162</a></div></div>
</div>
<div id="ref-dwtrWar2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">12. </div><div class="csl-right-inline"><strong>Defining Research Reproducibility: What Do You Mean?</strong> <div class="csl-block">David Mendoza, Christopher A Garcia</div> <em>Clinical Chemistry</em> (2017-11-01) <a href="https://doi.org/gq33c8">https://doi.org/gq33c8</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1373/clinchem.2017.279984">10.1373/clinchem.2017.279984</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/32100823">32100823</a></div></div>
</div>
<div id="ref-J1RPpvSe" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">13. </div><div class="csl-right-inline"><strong>What does research reproducibility mean?</strong> <div class="csl-block">Steven N Goodman, Daniele Fanelli, John PA Ioannidis</div> <em>Science Translational Medicine</em> (2016-06) <a href="https://doi.org/gc5sjs">https://doi.org/gc5sjs</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1126/scitranslmed.aaf5027">10.1126/scitranslmed.aaf5027</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27252173">27252173</a></div></div>
</div>
<div id="ref-1AET2xGSB" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">14. </div><div class="csl-right-inline"><strong>A statistical definition for reproducibility and replicability</strong> <div class="csl-block">Prasad Patil, Roger D Peng, Jeffrey T Leek</div> <em>Cold Spring Harbor Laboratory</em> (2016-07-29) <a href="https://doi.org/gftqhv">https://doi.org/gftqhv</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/066803">10.1101/066803</a></div></div>
</div>
<div id="ref-kqrki6Ml" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">15. </div><div class="csl-right-inline"><strong>Reproducibility vs. Replicability: A Brief History of a Confused Terminology</strong> <div class="csl-block">Hans E Plesser</div> <em>Frontiers in Neuroinformatics</em> (2018-01-18) <a href="https://doi.org/gc5ptr">https://doi.org/gc5ptr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.3389/fninf.2017.00076">10.3389/fninf.2017.00076</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29403370">29403370</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5778115">PMC5778115</a></div></div>
</div>
<div id="ref-1BInrYmTW" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">16. </div><div class="csl-right-inline"><strong>Reproducible Research in Computational Harmonic Analysis</strong> <div class="csl-block">David L Donoho, Arian Maleki, Inam Ur Rahman, Morteza Shahram, Victoria Stodden</div> <em>Computing in Science &amp;amp; Engineering</em> (2009-01) <a href="https://doi.org/fgmqfh">https://doi.org/fgmqfh</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/mcse.2009.15">10.1109/mcse.2009.15</a></div></div>
</div>
<div id="ref-1oH3MHiP" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">17. </div><div class="csl-right-inline"><strong>Why trust science?</strong> <div class="csl-block">Naomi Oreskes</div> <em>Princeton University Press</em> (2021) <div class="csl-block">ISBN: 9780691212265</div></div>
</div>
<div id="ref-g8wFsDHa" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">18. </div><div class="csl-right-inline"><strong>The logic of scientific discovery</strong> <div class="csl-block">Karl Popper</div> <em>Routledge</em> (2010) <div class="csl-block">ISBN: 9780415278447</div></div>
</div>
<div id="ref-Hopbpt36" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">19. </div><div class="csl-right-inline"><strong>Computational Reproducibility: The Elephant in the Room</strong> <div class="csl-block">Les Hatton, Michiel van Genuchten</div> <em>IEEE Software</em> (2019-03) <a href="https://doi.org/ggkvtr">https://doi.org/ggkvtr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/ms.2018.2883805">10.1109/ms.2018.2883805</a></div></div>
</div>
<div id="ref-JWFp2M7z" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">20. </div><div class="csl-right-inline"><strong>Reproducibility in Scientific Computing</strong> <div class="csl-block">Peter Ivie, Douglas Thain</div> <em>ACM Computing Surveys</em> (2019-05-31) <a href="https://doi.org/ggcqvn">https://doi.org/ggcqvn</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/3186266">10.1145/3186266</a></div></div>
</div>
<div id="ref-FsxIdYsZ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">21. </div><div class="csl-right-inline"><strong>Raise standards for preclinical cancer research</strong> <div class="csl-block">CGlenn Begley, Lee M Ellis</div> <em>Nature</em> (2012-03-28) <a href="https://doi.org/gd3xdh">https://doi.org/gd3xdh</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/483531a">10.1038/483531a</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22460880">22460880</a></div></div>
</div>
<div id="ref-yWY8AHZe" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">22. </div><div class="csl-right-inline"><strong>Believe it or not: how much can we rely on published data on potential drug targets?</strong> <div class="csl-block">Florian Prinz, Thomas Schlange, Khusru Asadullah</div> <em>Nature Reviews Drug Discovery</em> (2011-08-31) <a href="https://doi.org/dfsxxb">https://doi.org/dfsxxb</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nrd3439-c1">10.1038/nrd3439-c1</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/21892149">21892149</a></div></div>
</div>
<div id="ref-144brp8lV" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">23. </div><div class="csl-right-inline"><strong>Repeatability in computer systems research</strong> <div class="csl-block">Christian Collberg, Todd A Proebsting</div> <em>Communications of the ACM</em> (2016-02-25) <a href="https://doi.org/ggfp8k">https://doi.org/ggfp8k</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/2812803">10.1145/2812803</a></div></div>
</div>
<div id="ref-80e7T2Rv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">24. </div><div class="csl-right-inline"><strong>Editors’ Introduction to the Special Section on Replicability in Psychological Science</strong> <div class="csl-block">Harold Pashler, Eric–Jan Wagenmakers</div> <em>Perspectives on Psychological Science</em> (2012-11) <a href="https://doi.org/gckf56">https://doi.org/gckf56</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1177/1745691612465253">10.1177/1745691612465253</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26168108">26168108</a></div></div>
</div>
<div id="ref-tPSSxxsX" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">25. </div><div class="csl-right-inline"><strong>Reproducible Research in Computational Science</strong> <div class="csl-block">Roger D Peng</div> <em>Science</em> (2011-12-02) <a href="https://doi.org/fdv356">https://doi.org/fdv356</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1126/science.1213847">10.1126/science.1213847</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22144613">22144613</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3383002">PMC3383002</a></div></div>
</div>
<div id="ref-qs7B1fgV" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">26. </div><div class="csl-right-inline"><strong>Machine learning approaches to predict lupus disease activity from gene expression data</strong> <div class="csl-block">Brian Kegerreis, Michelle D Catalina, Prathyusha Bachali, Nicholas S Geraci, Adam C Labonte, Chen Zeng, Nathaniel Stearrett, Keith A Crandall, Peter E Lipsky, Amrie C Grammer</div> <em>Scientific Reports</em> (2019-07-03) <a href="https://doi.org/gh33ng">https://doi.org/gh33ng</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41598-019-45989-0">10.1038/s41598-019-45989-0</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/31270349">31270349</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6610624">PMC6610624</a></div></div>
</div>
<div id="ref-19eIjTcxN" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">27. </div><div class="csl-right-inline"><strong>Weighted elastic net for unsupervised domain adaptation with application to age prediction from DNA methylation data</strong> <div class="csl-block">Lisa Handl, Adrin Jalali, Michael Scherer, Ralf Eggeling, Nico Pfeifer</div> <em>Bioinformatics</em> (2019-07) <a href="https://doi.org/gf5d8b">https://doi.org/gf5d8b</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/bioinformatics/btz338">10.1093/bioinformatics/btz338</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/31510704">31510704</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6612879">PMC6612879</a></div></div>
</div>
<div id="ref-14mY2pXKf" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">28. </div><div class="csl-right-inline"><strong>UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</strong> <div class="csl-block">Leland McInnes, John Healy, James Melville</div> <em>arXiv</em> (2018) <a href="https://doi.org/gqzqzn">https://doi.org/gqzqzn</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1802.03426">10.48550/arxiv.1802.03426</a></div></div>
</div>
<div id="ref-IRSA7yBm" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">29. </div><div class="csl-right-inline"><strong>Deep-learning approach to identifying cancer subtypes using high-dimensional genomic data</strong> <div class="csl-block">Runpu Chen, Le Yang, Steve Goodison, Yijun Sun</div> <em>Bioinformatics</em> (2019-10-11) <a href="https://doi.org/gpfzxm">https://doi.org/gpfzxm</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/bioinformatics/btz769">10.1093/bioinformatics/btz769</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/31603461">31603461</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8215925">PMC8215925</a></div></div>
</div>
<div id="ref-7T8Uxprz" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">30. </div><div class="csl-right-inline"><strong>Applications of liquid biopsies for cancer</strong> <div class="csl-block">Austin K Mattox, Chetan Bettegowda, Shibin Zhou, Nickolas Papadopoulos, Kenneth W Kinzler, Bert Vogelstein</div> <em>Science Translational Medicine</em> (2019-08-28) <a href="https://doi.org/gjsfw9">https://doi.org/gjsfw9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1126/scitranslmed.aay1984">10.1126/scitranslmed.aay1984</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/31462507">31462507</a></div></div>
</div>
<div id="ref-Lphv6pyr" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">31. </div><div class="csl-right-inline"><strong>Histopathologic variables predict Oncotype DX™ Recurrence Score</strong> <div class="csl-block">Melina B Flanagan, David J Dabbs, Adam M Brufsky, Sushil Beriwal, Rohit Bhargava</div> <em>Modern Pathology</em> (2008-03-21) <a href="https://doi.org/d27rv3">https://doi.org/d27rv3</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/modpathol.2008.54">10.1038/modpathol.2008.54</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/18360352">18360352</a></div></div>
</div>
<div id="ref-dx78bdYB" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">32. </div><div class="csl-right-inline"><strong>Analysis of blood-based gene expression in idiopathic Parkinson disease</strong> <div class="csl-block">Ron Shamir, Christine Klein, David Amar, Eva-Juliane Vollstedt, Michael Bonin, Marija Usenovic, Yvette C Wong, Ales Maver, Sven Poths, Hershel Safer, … Dimitri Krainc</div> <em>Neurology</em> (2017-09-15) <a href="https://doi.org/gcnb67">https://doi.org/gcnb67</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1212/wnl.0000000000004516">10.1212/wnl.0000000000004516</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28916538">28916538</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5644465">PMC5644465</a></div></div>
</div>
<div id="ref-1CNHzMFjN" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">33. </div><div class="csl-right-inline"><strong>Blood Transcriptional Biomarkers for Active Tuberculosis among Patients in the United States: a Case-Control Study with Systematic Cross-Classifier Evaluation</strong> <div class="csl-block">Nicholas D Walter, Mikaela A Miller, Joshua Vasquez, Marc Weiner, Adam Chapman, Melissa Engle, Michael Higgins, Amy M Quinones, Vanessa Rosselli, Elizabeth Canono, … Mark W Geraci</div> <em>Journal of Clinical Microbiology</em> (2016-02) <a href="https://doi.org/gqzq2r">https://doi.org/gqzq2r</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1128/jcm.01990-15">10.1128/jcm.01990-15</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26582831">26582831</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4733166">PMC4733166</a></div></div>
</div>
<div id="ref-rJ5EpmYl" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">34. </div><div class="csl-right-inline"><strong>A Transcriptomic Biomarker to Quantify Systemic Inflammation in Sepsis — A Prospective Multicenter Phase II Diagnostic Study</strong> <div class="csl-block">Michael Bauer, Evangelos J Giamarellos-Bourboulis, Andreas Kortgen, Eva Möller, Karen Felsmann, Jean Marc Cavaillon, Orlando Guntinas-Lichius, Olivier Rutschmann, Andriy Ruryk, Matthias Kohl, … Konrad Reinhart</div> <em>EBioMedicine</em> (2016-04) <a href="https://doi.org/gqzq2q">https://doi.org/gqzq2q</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.ebiom.2016.03.006">10.1016/j.ebiom.2016.03.006</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27211554">27211554</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4856796">PMC4856796</a></div></div>
</div>
<div id="ref-1ZboiGsF" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">35. </div><div class="csl-right-inline"><strong>Gene expression profiling of peripheral blood from patients with untreated new-onset systemic juvenile idiopathic arthritis reveals molecular heterogeneity that may predict macrophage activation syndrome</strong> <div class="csl-block">Ndate Fall, Michael Barnes, Sherry Thornton, Lorie Luyrink, Judyann Olson, Norman T Ilowite, Beth S Gottlieb, Thomas Griffin, David D Sherry, Susan Thompson, … Alexei A Grom</div> <em>Arthritis &amp;amp; Rheumatism</em> (2007) <a href="https://doi.org/chxcfh">https://doi.org/chxcfh</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1002/art.22981">10.1002/art.22981</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/17968951">17968951</a></div></div>
</div>
<div id="ref-J9zGDTW4" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">36. </div><div class="csl-right-inline"><strong>Light-Directed, Spatially Addressable Parallel Chemical Synthesis</strong> <div class="csl-block">Stephen PA Fodor, JLeighton Read, Michael C Pirrung, Lubert Stryer, Amy Tsai Lu, Dennis Solas</div> <em>Science</em> (1991-02-15) <a href="https://doi.org/dw6f5b">https://doi.org/dw6f5b</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1126/science.1990438">10.1126/science.1990438</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/1990438">1990438</a></div></div>
</div>
<div id="ref-XeqEIk0K" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">37. </div><div class="csl-right-inline"><strong>Expression monitoring by hybridization to high-density oligonucleotide arrays</strong> <div class="csl-block">David J Lockhart, Helin Dong, Michael C Byrne, Maximillian T Follettie, Michael V Gallo, Mark S Chee, Michael Mittmann, Chunwei Wang, Michiko Kobayashi, Heidi Norton, Eugene L Brown</div> <em>Nature Biotechnology</em> (1996-12) <a href="https://doi.org/bpmwzt">https://doi.org/bpmwzt</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nbt1296-1675">10.1038/nbt1296-1675</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/9634850">9634850</a></div></div>
</div>
<div id="ref-ChQys9ED" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">38. </div><div class="csl-right-inline"><strong>Comparison of RNA-Seq and Microarray in Transcriptome Profiling of Activated T Cells</strong> <div class="csl-block">Shanrong Zhao, Wai-Ping Fung-Leung, Anton Bittner, Karen Ngo, Xuejun Liu</div> <em>PLoS ONE</em> (2014-01-16) <a href="https://doi.org/f5tvg3">https://doi.org/f5tvg3</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1371/journal.pone.0078644">10.1371/journal.pone.0078644</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24454679">24454679</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3894192">PMC3894192</a></div></div>
</div>
<div id="ref-fVSo2gZU" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">39. </div><div class="csl-right-inline"><strong>ImageNet classification with deep convolutional neural networks</strong> <div class="csl-block">Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton</div> <em>Communications of the ACM</em> (2017-05-24) <a href="https://doi.org/gbhhxs">https://doi.org/gbhhxs</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1145/3065386">10.1145/3065386</a></div></div>
</div>
<div id="ref-c1tQ2yNq" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">40. </div><div class="csl-right-inline"><strong>Understanding Back-Translation at Scale</strong> <div class="csl-block">Sergey Edunov, Myle Ott, Michael Auli, David Grangier</div> <em>arXiv</em> (2018) <a href="https://doi.org/gqzq2v">https://doi.org/gqzq2v</a> <div class="csl-block">DOI: <a href="https://doi.org/10.48550/arxiv.1808.09381">10.48550/arxiv.1808.09381</a></div></div>
</div>
<div id="ref-HpZqLKUe" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">41. </div><div class="csl-right-inline"><strong>Exploring the Limits of Transfer Learning with a Unified Text-to-Text
Transformer</strong> <div class="csl-block">Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu</div> <em>arXiv</em> (2019-10-23) <a href="https://arxiv.org/abs/1910.10683v3">https://arxiv.org/abs/1910.10683v3</a></div>
</div>
<div id="ref-ld1EnZ6i" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">42. </div><div class="csl-right-inline"><strong>U-Net: Convolutional Networks for Biomedical Image Segmentation</strong> <div class="csl-block">Olaf Ronneberger, Philipp Fischer, Thomas Brox</div> <em>arXiv</em> (2015-05-19) <a href="https://arxiv.org/abs/1505.04597">https://arxiv.org/abs/1505.04597</a></div>
</div>
<div id="ref-MzuxpiPn" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">43. </div><div class="csl-right-inline"><strong>A review of feature selection methods based on mutual information</strong> <div class="csl-block">Jorge R Vergara, Pablo A Estévez</div> <em>Neural Computing and Applications</em> (2013-03-13) <a href="https://doi.org/gj7fzd">https://doi.org/gj7fzd</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/s00521-013-1368-0">10.1007/s00521-013-1368-0</a></div></div>
</div>
<div id="ref-Vzg9ivZU" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">44. </div><div class="csl-right-inline"><strong>A comparative study of feature selection and multiclass classification methods for tissue classification based on gene expression</strong> <div class="csl-block">T Li, C Zhang, M Ogihara</div> <em>Bioinformatics</em> (2004-04-15) <a href="https://doi.org/b3kzzp">https://doi.org/b3kzzp</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/bioinformatics/bth267">10.1093/bioinformatics/bth267</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/15087314">15087314</a></div></div>
</div>
<div id="ref-QSJEewCc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">45. </div><div class="csl-right-inline"><strong>COSMIC (the Catalogue of Somatic Mutations in Cancer): a resource to investigate acquired mutations in human cancer</strong> <div class="csl-block">Simon A Forbes, Gurpreet Tang, Nidhi Bindal, Sally Bamford, Elisabeth Dawson, Charlotte Cole, Chai Yin Kok, Mingming Jia, Rebecca Ewing, Andrew Menzies, … PAndrew Futreal</div> <em>Nucleic Acids Research</em> (2009-11-10) <a href="https://doi.org/fhkk8s">https://doi.org/fhkk8s</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/nar/gkp995">10.1093/nar/gkp995</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/19906727">19906727</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2808858">PMC2808858</a></div></div>
</div>
<div id="ref-ClFEdqWg" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">46. </div><div class="csl-right-inline"><strong>Application of a Neural Network Whole Transcriptome–Based Pan-Cancer Method for Diagnosis of Primary and Metastatic Cancers</strong> <div class="csl-block">Jasleen K Grewal, Basile Tessier-Cloutier, Martin Jones, Sitanshu Gakkhar, Yussanne Ma, Richard Moore, Andrew J Mungall, Yongjun Zhao, Michael D Taylor, Karen Gelmon, … Steven JM Jones</div> <em>JAMA Network Open</em> (2019-04-26) <a href="https://doi.org/gf84h2">https://doi.org/gf84h2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1001/jamanetworkopen.2019.2597">10.1001/jamanetworkopen.2019.2597</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/31026023">31026023</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6487574">PMC6487574</a></div></div>
</div>
<div id="ref-F7lIlh2N" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">47. </div><div class="csl-right-inline"><strong>A Next Generation Connectivity Map: L1000 Platform and the First 1,000,000 Profiles</strong> <div class="csl-block">Aravind Subramanian, Rajiv Narayan, Steven M Corsello, David D Peck, Ted E Natoli, Xiaodong Lu, Joshua Gould, John F Davis, Andrew A Tubelli, Jacob K Asiedu, … Todd R Golub</div> <em>Cell</em> (2017-11) <a href="https://doi.org/cgwt">https://doi.org/cgwt</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.cell.2017.10.049">10.1016/j.cell.2017.10.049</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29195078">29195078</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5990023">PMC5990023</a></div></div>
</div>
<div id="ref-12QQw9p7v" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">48. </div><div class="csl-right-inline"><strong>Gene expression inference with deep learning</strong> <div class="csl-block">Yifei Chen, Yi Li, Rajiv Narayan, Aravind Subramanian, Xiaohui Xie</div> <em>Bioinformatics</em> (2016-02-11) <a href="https://doi.org/f8vmtt">https://doi.org/f8vmtt</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/bioinformatics/btw074">10.1093/bioinformatics/btw074</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/26873929">26873929</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4908320">PMC4908320</a></div></div>
</div>
<div id="ref-J0eeAld3" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">49. </div><div class="csl-right-inline"><strong>Robust clinical outcome prediction based on Bayesian analysis of transcriptional profiles and prior causal networks</strong> <div class="csl-block">Kourosh Zarringhalam, Ahmed Enayetallah, Padmalatha Reddy, Daniel Ziemek</div> <em>Bioinformatics</em> (2014-06-11) <a href="https://doi.org/f58bp2">https://doi.org/f58bp2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/bioinformatics/btu272">10.1093/bioinformatics/btu272</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/24932007">24932007</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4058945">PMC4058945</a></div></div>
</div>
<div id="ref-g8OoyIPj" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">50. </div><div class="csl-right-inline"><strong>Robust phenotype prediction from gene expression data using differential shrinkage of co-regulated genes</strong> <div class="csl-block">Kourosh Zarringhalam, David Degras, Christoph Brockel, Daniel Ziemek</div> <em>Scientific Reports</em> (2018-01-19) <a href="https://doi.org/gcwzdn">https://doi.org/gcwzdn</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41598-018-19635-0">10.1038/s41598-018-19635-0</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29352257">29352257</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5775343">PMC5775343</a></div></div>
</div>
<div id="ref-IHi1L0uN" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">51. </div><div class="csl-right-inline"><strong>Prognostic gene signatures for patient stratification in breast cancer - accuracy, stability and interpretability of gene selection approaches using prior knowledge on protein-protein interactions</strong> <div class="csl-block">Yupeng Cun, Holger Fröhlich</div> <em>BMC Bioinformatics</em> (2012-05-01) <a href="https://doi.org/f4cb5r">https://doi.org/f4cb5r</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1186/1471-2105-13-69">10.1186/1471-2105-13-69</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22548963">22548963</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3436770">PMC3436770</a></div></div>
</div>
<div id="ref-Ki2ij7zE" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">52. </div><div class="csl-right-inline"><strong>Pathway-level information extractor (PLIER) for gene expression data</strong> <div class="csl-block">Weiguang Mao, Elena Zaslavsky, Boris M Hartmann, Stuart C Sealfon, Maria Chikina</div> <em>Nature Methods</em> (2019-06-27) <a href="https://doi.org/gf75g6">https://doi.org/gf75g6</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41592-019-0456-1">10.1038/s41592-019-0456-1</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/31249421">31249421</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7262669">PMC7262669</a></div></div>
</div>
<div id="ref-14rnBunuZ" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">53. </div><div class="csl-right-inline"><strong>MultiPLIER: A Transfer Learning Framework for Transcriptomics Reveals Systemic Features of Rare Disease</strong> <div class="csl-block">Jaclyn N Taroni, Peter C Grayson, Qiwen Hu, Sean Eddy, Matthias Kretzler, Peter A Merkel, Casey S Greene</div> <em>Cell Systems</em> (2019-05) <a href="https://doi.org/gf75g5">https://doi.org/gf75g5</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.cels.2019.04.003">10.1016/j.cels.2019.04.003</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/31121115">31121115</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6538307">PMC6538307</a></div></div>
</div>
<div id="ref-2i0SZtHv" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">54. </div><div class="csl-right-inline"><strong>Transfer Learning for Molecular Cancer Classification Using Deep Neural Networks</strong> <div class="csl-block">Rahul K Sevakula, Vikas Singh, Nishchal K Verma, Chandan Kumar, Yan Cui</div> <em>IEEE/ACM Transactions on Computational Biology and Bioinformatics</em> (2019-11-01) <a href="https://doi.org/gqzq3p">https://doi.org/gqzq3p</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/tcbb.2018.2822803">10.1109/tcbb.2018.2822803</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29993662">29993662</a></div></div>
</div>
<div id="ref-uXRnjUvq" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">55. </div><div class="csl-right-inline"><strong>A semi-supervised deep learning method based on stacked sparse auto-encoder for cancer prediction using RNA-seq data</strong> <div class="csl-block">Yawen Xiao, Jun Wu, Zongli Lin, Xiaodong Zhao</div> <em>Computer Methods and Programs in Biomedicine</em> (2018-11) <a href="https://doi.org/gfnm5c">https://doi.org/gfnm5c</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.cmpb.2018.10.004">10.1016/j.cmpb.2018.10.004</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/30415723">30415723</a></div></div>
</div>
<div id="ref-bBLm1pxP" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">56. </div><div class="csl-right-inline"><div class="csl-block">Igor Kononenko, Edvard Šimec, Marko Robnik-Šikonja</div> <em>Applied Intelligence</em> (1997) <a href="https://doi.org/fdm4r3">https://doi.org/fdm4r3</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1023/a:1008280620621">10.1023/a:1008280620621</a></div></div>
</div>
<div id="ref-17srxvYZd" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">57. </div><div class="csl-right-inline"><strong>Application of transfer learning for cancer drug sensitivity prediction</strong> <div class="csl-block">Saugato Rahman Dhruba, Raziur Rahman, Kevin Matlock, Souparno Ghosh, Ranadip Pal</div> <em>BMC Bioinformatics</em> (2018-12) <a href="https://doi.org/gh4mnw">https://doi.org/gh4mnw</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1186/s12859-018-2465-y">10.1186/s12859-018-2465-y</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/30591023">30591023</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6309077">PMC6309077</a></div></div>
</div>
<div id="ref-jxCyAK09" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">58. </div><div class="csl-right-inline"><strong>A comprehensive genomic pan-cancer classification using The Cancer Genome Atlas gene expression data</strong> <div class="csl-block">Yuanyuan Li, Kai Kang, Juno M Krahn, Nicole Croutwater, Kevin Lee, David M Umbach, Leping Li</div> <em>BMC Genomics</em> (2017-07-03) <a href="https://doi.org/gfv37q">https://doi.org/gfv37q</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1186/s12864-017-3906-0">10.1186/s12864-017-3906-0</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28673244">28673244</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5496318">PMC5496318</a></div></div>
</div>
<div id="ref-CD9ApROI" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">59. </div><div class="csl-right-inline"><strong>Gene expression microarray classification using PCA–BEL</strong> <div class="csl-block">Ehsan Lotfi, Azita Keshavarz</div> <em>Computers in Biology and Medicine</em> (2014-11) <a href="https://doi.org/gqzq5p">https://doi.org/gqzq5p</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.compbiomed.2014.09.008">10.1016/j.compbiomed.2014.09.008</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25282708">25282708</a></div></div>
</div>
<div id="ref-NLVTJ9Lj" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">60. </div><div class="csl-right-inline"><strong>Auto-Encoding Variational Bayes</strong> <div class="csl-block">Diederik P Kingma, Max Welling</div> <em>arXiv</em> (2014-05-02) <a href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</a></div>
</div>
<div id="ref-14bFfy6t2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">61. </div><div class="csl-right-inline"><strong>Higher Order Contractive Auto-Encoder</strong> <div class="csl-block">Salah Rifai, Grégoire Mesnil, Pascal Vincent, Xavier Muller, Yoshua Bengio, Yann Dauphin, Xavier Glorot</div> <em>Machine Learning and Knowledge Discovery in Databases</em> (2011) <a href="https://doi.org/bfpkgr">https://doi.org/bfpkgr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/978-3-642-23783-6_41">10.1007/978-3-642-23783-6_41</a></div></div>
</div>
<div id="ref-z3sjep1J" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">62. </div><div class="csl-right-inline"><strong>DeePathology: Deep Multi-Task Learning for Inferring Molecular Pathology from Cancer Transcriptome</strong> <div class="csl-block">Behrooz Azarkhalili, Ali Saberi, Hamidreza Chitsaz, Ali Sharifi-Zarchi</div> <em>Scientific Reports</em> (2019-11-11) <a href="https://doi.org/gpg7vc">https://doi.org/gpg7vc</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41598-019-52937-5">10.1038/s41598-019-52937-5</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/31712594">31712594</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6848155">PMC6848155</a></div></div>
</div>
<div id="ref-11ZzdspWP" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">63. </div><div class="csl-right-inline"><strong>A DEEP LEARNING APPROACH FOR CANCER DETECTION AND RELEVANT GENE IDENTIFICATION</strong> <div class="csl-block">PADIDEH DANAEE, REZA GHAEINI, DAVID A HENDRIX</div> <em>Biocomputing 2017</em> (2016-11-22) <a href="https://doi.org/gqzq5q">https://doi.org/gqzq5q</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1142/9789813207813_0022">10.1142/9789813207813_0022</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27896977">27896977</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5177447">PMC5177447</a></div></div>
</div>
<div id="ref-1CPSh19Mn" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">64. </div><div class="csl-right-inline"><strong>Exploring single-cell data with deep multitasking neural networks</strong> <div class="csl-block">Matthew Amodio, David van Dijk, Krishnan Srinivasan, William S Chen, Hussein Mohsen, Kevin R Moon, Allison Campbell, Yujiao Zhao, Xiaomei Wang, Manjunatha Venkataswamy, … Smita Krishnaswamy</div> <em>Nature Methods</em> (2019-10-07) <a href="https://doi.org/gf9rsg">https://doi.org/gf9rsg</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41592-019-0576-7">10.1038/s41592-019-0576-7</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/31591579">31591579</a></div></div>
</div>
<div id="ref-UPHmb9yc" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">65. </div><div class="csl-right-inline"><strong>Detecting and correcting systematic variation in large-scale RNA sequencing data</strong> <div class="csl-block">Sheng Li, Paweł P Łabaj, Paul Zumbo, Peter Sykacek, Wei Shi, Leming Shi, John Phan, Po-Yen Wu, May Wang, Charles Wang, … Christopher E Mason</div> <em>Nature Biotechnology</em> (2014-08-24) <a href="https://doi.org/f6j2gj">https://doi.org/f6j2gj</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/nbt.3000">10.1038/nbt.3000</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/25150837">25150837</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4160374">PMC4160374</a></div></div>
</div>
<div id="ref-rYMEu6oz" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">66. </div><div class="csl-right-inline"><strong>Machine Learning Identifies Stemness Features Associated with Oncogenic Dedifferentiation</strong> <div class="csl-block">Tathiane M Malta, Artem Sokolov, Andrew J Gentles, Tomasz Burzykowski, Laila Poisson, John N Weinstein, Bożena Kamińska, Joerg Huelsken, Larsson Omberg, Olivier Gevaert, … Armaz Mariamidze</div> <em>Cell</em> (2018-04) <a href="https://doi.org/gc93hh">https://doi.org/gc93hh</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.cell.2018.03.034">10.1016/j.cell.2018.03.034</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29625051">29625051</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5902191">PMC5902191</a></div></div>
</div>
<div id="ref-fZGWNG2y" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">67. </div><div class="csl-right-inline"><strong>&amp;lt;p&amp;gt;The blood transcriptional signature for active and latent tuberculosis&amp;lt;/p&amp;gt;</strong> <div class="csl-block">Min Deng, Xiao-Dong Lv, Zhi-Xian Fang, Xin-Sheng Xie, Wen-Yu Chen</div> <em>Infection and Drug Resistance</em> (2019-01) <a href="https://doi.org/gqzq6v">https://doi.org/gqzq6v</a> <div class="csl-block">DOI: <a href="https://doi.org/10.2147/idr.s184640">10.2147/idr.s184640</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/30787624">30787624</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6363485">PMC6363485</a></div></div>
</div>
<div id="ref-1HahRBkyb" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">68. </div><div class="csl-right-inline"><strong>Adjusting batch effects in microarray expression data using empirical Bayes methods</strong> <div class="csl-block">WEvan Johnson, Cheng Li, Ariel Rabinovic</div> <em>Biostatistics</em> (2006-04-21) <a href="https://doi.org/dsf386">https://doi.org/dsf386</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/biostatistics/kxj037">10.1093/biostatistics/kxj037</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/16632515">16632515</a></div></div>
</div>
<div id="ref-EMrWUv3D" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">69. </div><div class="csl-right-inline"><strong>Don’t Rule Out Simple Models Prematurely: A Large Scale Benchmark Comparing Linear and Non-linear Classifiers in OpenML</strong> <div class="csl-block">Benjamin Strang, Peter van der Putten, Jan N van Rijn, Frank Hutter</div> <em>Advances in Intelligent Data Analysis XVII</em> (2018) <a href="https://doi.org/gqzq6q">https://doi.org/gqzq6q</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/978-3-030-01768-2_25">10.1007/978-3-030-01768-2_25</a></div></div>
</div>
<div id="ref-GE6d2dOD" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">70. </div><div class="csl-right-inline"><strong>Does deep learning always outperform simple linear regression in optical imaging?</strong> <div class="csl-block">Shuming Jiao, Yang Gao, Jun Feng, Ting Lei, Xiaocong Yuan</div> <em>arXiv</em> (2020-02-19) <a href="https://arxiv.org/abs/1911.00353">https://arxiv.org/abs/1911.00353</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1364/OE.382319">10.1364/oe.382319</a></div></div>
</div>
<div id="ref-xSV2BrbO" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">71. </div><div class="csl-right-inline"><strong>Beyond the hype: deep neural networks outperform established methods using a ChEMBL bioactivity benchmark set</strong> <div class="csl-block">Eelke B Lenselink, Niels ten Dijke, Brandon Bongers, George Papadatos, Herman WT van Vlijmen, Wojtek Kowalczyk, Adriaan P IJzerman, Gerard JP van Westen</div> <em>Journal of Cheminformatics</em> (2017-08-14) <a href="https://doi.org/gbwq98">https://doi.org/gbwq98</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1186/s13321-017-0232-0">10.1186/s13321-017-0232-0</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29086168">29086168</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5555960">PMC5555960</a></div></div>
</div>
<div id="ref-15s31cve5" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">72. </div><div class="csl-right-inline"><strong>Benchmarking deep learning models on large healthcare datasets</strong> <div class="csl-block">Sanjay Purushotham, Chuizheng Meng, Zhengping Che, Yan Liu</div> <em>Journal of Biomedical Informatics</em> (2018-07) <a href="https://doi.org/gd97qc">https://doi.org/gd97qc</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.jbi.2018.04.007">10.1016/j.jbi.2018.04.007</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/29879470">29879470</a></div></div>
</div>
<div id="ref-KE9BUIOX" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">73. </div><div class="csl-right-inline"><strong>I tried a bunch of things: The dangers of unexpected overfitting in classification of brain data</strong> <div class="csl-block">Mahan Hosseini, Michael Powell, John Collins, Chloe Callahan-Flintoft, William Jones, Howard Bowman, Brad Wyble</div> <em>Neuroscience &amp;amp; Biobehavioral Reviews</em> (2020-12) <a href="https://doi.org/ghkskv">https://doi.org/ghkskv</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.neubiorev.2020.09.036">10.1016/j.neubiorev.2020.09.036</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/33035522">33035522</a></div></div>
</div>
<div id="ref-V3nGUaio" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">74. </div><div class="csl-right-inline"><strong>Massive single-cell RNA-seq analysis and imputation via deep learning</strong> <div class="csl-block">Yue Deng, Feng Bao, Qionghai Dai, Lani F Wu, Steven J Altschuler</div> <em>Cold Spring Harbor Laboratory</em> (2018-05-06) <a href="https://doi.org/gfgrpm">https://doi.org/gfgrpm</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1101/315556">10.1101/315556</a></div></div>
</div>
<div id="ref-Uot2y2ws" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">75. </div><div class="csl-right-inline"><strong>A global immune gene expression signature for human cancers</strong> <div class="csl-block">Yuexin Liu</div> <em>Oncotarget</em> (2019-03-08) <a href="https://doi.org/gqzq8j">https://doi.org/gqzq8j</a> <div class="csl-block">DOI: <a href="https://doi.org/10.18632/oncotarget.26773">10.18632/oncotarget.26773</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/30956779">30956779</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6443003">PMC6443003</a></div></div>
</div>
<div id="ref-7k4Mlul7" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">76. </div><div class="csl-right-inline"><strong>A Four-Biomarker Blood Signature Discriminates Systemic Inflammation Due to Viral Infection Versus Other Etiologies</strong> <div class="csl-block">DL Sampson, BA Fox, TD Yager, S Bhide, S Cermelli, LC McHugh, TA Seldon, RA Brandon, E Sullivan, JJ Zimmerman, … RB Brandon</div> <em>Scientific Reports</em> (2017-06-06) <a href="https://doi.org/gc4zdw">https://doi.org/gc4zdw</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41598-017-02325-8">10.1038/s41598-017-02325-8</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/28588308">28588308</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5460227">PMC5460227</a></div></div>
</div>
<div id="ref-URjcKCcA" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">77. </div><div class="csl-right-inline"><strong>Multitask learning improves prediction of cancer drug sensitivity</strong> <div class="csl-block">Han Yuan, Ivan Paskov, Hristo Paskov, Alvaro J González, Christina S Leslie</div> <em>Scientific Reports</em> (2016-08-23) <a href="https://doi.org/f8zbhk">https://doi.org/f8zbhk</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/srep31619">10.1038/srep31619</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27550087">27550087</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4994023">PMC4994023</a></div></div>
</div>
<div id="ref-1DeCFT6PA" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">78. </div><div class="csl-right-inline"><strong>Invariant Risk Minimization</strong> <div class="csl-block">Martin Arjovsky, Léon Bottou, Ishaan Gulrajani, David Lopez-Paz</div> <em>arXiv</em> (2020-03-31) <a href="https://arxiv.org/abs/1907.02893">https://arxiv.org/abs/1907.02893</a></div>
</div>
<div id="ref-XR77uqve" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">79. </div><div class="csl-right-inline"><strong>Invariant Risk Minimization Games</strong> <div class="csl-block">Kartik Ahuja, Karthikeyan Shanmugam, Kush R Varshney, Amit Dhurandhar</div> <em>arXiv</em> (2020-03-20) <a href="https://arxiv.org/abs/2002.04692">https://arxiv.org/abs/2002.04692</a></div>
</div>
<div id="ref-mh4sXNBi" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">80. </div><div class="csl-right-inline"><strong>Supervised, semi-supervised and unsupervised inference of gene regulatory networks</strong> <div class="csl-block">SR Maetschke, PB Madhamshettiwar, MJ Davis, MA Ragan</div> <em>Briefings in Bioinformatics</em> (2013-05-21) <a href="https://doi.org/gccv5w">https://doi.org/gccv5w</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1093/bib/bbt034">10.1093/bib/bbt034</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/23698722">23698722</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3956069">PMC3956069</a></div></div>
</div>
<div id="ref-xdJRSgce" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">81. </div><div class="csl-right-inline"><strong>recount3: summaries and queries for large-scale RNA-seq expression and splicing</strong> <div class="csl-block">Christopher Wilks, Shijie C Zheng, Feng Yong Chen, Rone Charles, Brad Solomon, Jonathan P Ling, Eddie Luidy Imada, David Zhang, Lance Joseph, Jeffrey T Leek, … Ben Langmead</div> <em>Genome Biology</em> (2021-11-29) <a href="https://doi.org/gnm7zc">https://doi.org/gnm7zc</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1186/s13059-021-02533-6">10.1186/s13059-021-02533-6</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/34844637">34844637</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8628444">PMC8628444</a></div></div>
</div>
<div id="ref-DKDxhmok" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">82. </div><div class="csl-right-inline"><strong>An Overview of Multi-Task Learning in Deep Neural Networks</strong> <div class="csl-block">Sebastian Ruder</div> <em>arXiv</em> (2017-06-19) <a href="https://arxiv.org/abs/1706.05098">https://arxiv.org/abs/1706.05098</a></div>
</div>
<div id="ref-dmxNoeYV" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">83. </div><div class="csl-right-inline"><strong>Citation Indexes for Science</strong> <div class="csl-block">Eugene Garfield</div> <em>Science</em> (1955-07-15) <a href="https://doi.org/fnkc4f">https://doi.org/fnkc4f</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1126/science.122.3159.108">10.1126/science.122.3159.108</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/14385826">14385826</a></div></div>
</div>
<div id="ref-107JaW2qS" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">84. </div><div class="csl-right-inline"><strong>The science of science</strong> <div class="csl-block">Dashun Wang, Albert-László Barabási</div> <em>Cambridge University Press</em> (2021) <div class="csl-block">ISBN: 9781108492669</div></div>
</div>
<div id="ref-qXsETMbA" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">85. </div><div class="csl-right-inline"><strong>An index to quantify an individual's scientific research output</strong> <div class="csl-block">JE Hirsch</div> <em>Proceedings of the National Academy of Sciences</em> (2005-11-07) <a href="https://doi.org/cbq6dz">https://doi.org/cbq6dz</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1073/pnas.0507655102">10.1073/pnas.0507655102</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/16275915">16275915</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1283832">PMC1283832</a></div></div>
</div>
<div id="ref-1bkadVmO" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">86. </div><div class="csl-right-inline"><strong>The h -index Debate: An Introduction for Librarians</strong> <div class="csl-block">Cameron Barnes</div> <em>The Journal of Academic Librarianship</em> (2017-11) <a href="https://doi.org/gcjpk2">https://doi.org/gcjpk2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.acalib.2017.08.013">10.1016/j.acalib.2017.08.013</a></div></div>
</div>
<div id="ref-OsrIUdWt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">87. </div><div class="csl-right-inline"><strong>The h-index is no longer an effective correlate of scientific reputation</strong> <div class="csl-block">Vladlen Koltun, David Hafner</div> <em>PLOS ONE</em> (2021-06-28) <a href="https://doi.org/gkzfnr">https://doi.org/gkzfnr</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1371/journal.pone.0253397">10.1371/journal.pone.0253397</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/34181681">34181681</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8238192">PMC8238192</a></div></div>
</div>
<div id="ref-iDjOlq4q" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">88. </div><div class="csl-right-inline"><strong>Theory and practise of the g-index</strong> <div class="csl-block">Leo Egghe</div> <em>Scientometrics</em> (2006-10) <a href="https://doi.org/dgj3tc">https://doi.org/dgj3tc</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/s11192-006-0144-7">10.1007/s11192-006-0144-7</a></div></div>
</div>
<div id="ref-16V0td5f2" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">89. </div><div class="csl-right-inline"><strong>New Tools for Improving and Evaluating The Effectiveness of Research</strong> <div class="csl-block">Irving H Sher, Eugene Garfield</div> <em>Research Program Effectiveness</em> (1965-06-27)</div>
</div>
<div id="ref-ENhL9A6A" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">90. </div><div class="csl-right-inline"><strong>Eigenfactor: Measuring the value and prestige of scholarly journals</strong> <div class="csl-block">Carl Bergstrom</div> <em>College &amp; research libraries news</em> (2007)</div>
</div>
<div id="ref-5vaZVhmk" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">91. </div><div class="csl-right-inline"><strong>A systematic empirical comparison of different approaches for normalizing citation impact indicators</strong> <div class="csl-block">Ludo Waltman, Nees Jan van Eck</div> <em>Journal of Informetrics</em> (2013-10) <a href="https://doi.org/f5jdr5">https://doi.org/f5jdr5</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.joi.2013.08.002">10.1016/j.joi.2013.08.002</a></div></div>
</div>
<div id="ref-zeuTsDVX" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">92. </div><div class="csl-right-inline"><strong>The PageRank Citation Ranking: Bringing Order to the Web.</strong> <div class="csl-block">Lawrence Page, Sergey Brin, Rajeev Motwani, Terry Winograd</div> <em>Stanford InfoLab</em> (1999)</div>
</div>
<div id="ref-ZiPXgNU5" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">93. </div><div class="csl-right-inline"><strong>Maps of random walks on complex networks reveal community structure</strong> <div class="csl-block">Martin Rosvall, Carl T Bergstrom</div> <em>Proceedings of the National Academy of Sciences</em> (2008-01-29) <a href="https://doi.org/fw5xcm">https://doi.org/fw5xcm</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1073/pnas.0706851105">10.1073/pnas.0706851105</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/18216267">18216267</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2234100">PMC2234100</a></div></div>
</div>
<div id="ref-UTLLCTKy" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">94. </div><div class="csl-right-inline"><strong>Large teams develop and small teams disrupt science and technology</strong> <div class="csl-block">Lingfei Wu, Dashun Wang, James A Evans</div> <em>Nature</em> (2019-02) <a href="https://doi.org/gfvnb9">https://doi.org/gfvnb9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1038/s41586-019-0941-9">10.1038/s41586-019-0941-9</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/30760923">30760923</a></div></div>
</div>
<div id="ref-PGRGcmLi" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">95. </div><div class="csl-right-inline"><strong>Are disruption index indicators convergently valid? The comparison of several indicator variants with assessments by peers</strong> <div class="csl-block">Lutz Bornmann, Sitaram Devarakonda, Alexander Tekles, George Chacko</div> <em>Quantitative Science Studies</em> (2020-08) <a href="https://doi.org/gq2ts5">https://doi.org/gq2ts5</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1162/qss_a_00068">10.1162/qss_a_00068</a></div></div>
</div>
<div id="ref-qT77Z7V" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">96. </div><div class="csl-right-inline"><strong>Tradition and Innovation in Scientists’ Research Strategies</strong> <div class="csl-block">Jacob G Foster, Andrey Rzhetsky, James A Evans</div> <em>American Sociological Review</em> (2015-09-01) <a href="https://doi.org/f7tzm5">https://doi.org/f7tzm5</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1177/0003122415601618">10.1177/0003122415601618</a></div></div>
</div>
<div id="ref-RzTov9Er" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">97. </div><div class="csl-right-inline"><strong>Bias against novelty in science: A cautionary tale for users of bibliometric indicators</strong> <div class="csl-block">Jian Wang, Reinhilde Veugelers, Paula Stephan</div> <em>Research Policy</em> (2017-10) <a href="https://doi.org/gb22vw">https://doi.org/gb22vw</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.respol.2017.06.006">10.1016/j.respol.2017.06.006</a></div></div>
</div>
<div id="ref-kZI40ZXS" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">98. </div><div class="csl-right-inline"><strong>Relative Citation Ratio (RCR): A New Metric That Uses Citation Rates to Measure Influence at the Article Level</strong> <div class="csl-block">BIan Hutchins, Xin Yuan, James M Anderson, George M Santangelo</div> <em>PLOS Biology</em> (2016-09-06) <a href="https://doi.org/f88zk2">https://doi.org/f88zk2</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1371/journal.pbio.1002541">10.1371/journal.pbio.1002541</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/27599104">27599104</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5012559">PMC5012559</a></div></div>
</div>
<div id="ref-11IkPxdIL" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">99. </div><div class="csl-right-inline"><strong>Measuring contextual citation impact of scientific journals</strong> <div class="csl-block">Henk F Moed</div> <em>Journal of Informetrics</em> (2010-07) <a href="https://doi.org/dpbgj9">https://doi.org/dpbgj9</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.joi.2010.01.002">10.1016/j.joi.2010.01.002</a></div></div>
</div>
<div id="ref-12CYfIcWF" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">100. </div><div class="csl-right-inline"><strong>Collective topical PageRank: a model to evaluate the topic-dependent academic impact of scientific papers</strong> <div class="csl-block">Yongjun Zhang, Jialin Ma, Zijian Wang, Bolun Chen, Yongtao Yu</div> <em>Scientometrics</em> (2017-12-23) <a href="https://doi.org/gc4b2s">https://doi.org/gc4b2s</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/s11192-017-2626-1">10.1007/s11192-017-2626-1</a></div></div>
</div>
<div id="ref-4F5AdeXW" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">101. </div><div class="csl-right-inline"><strong>Quantifying and suppressing ranking bias in a large citation network</strong> <div class="csl-block">Giacomo Vaccario, Matus Medo, Nicolas Wider, Manuel Sebastian Mariani</div> <em>arXiv</em> (2017-08-30) <a href="https://arxiv.org/abs/1703.08071">https://arxiv.org/abs/1703.08071</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.joi.2017.05.014">10.1016/j.joi.2017.05.014</a></div></div>
</div>
<div id="ref-126JIm8me" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">102. </div><div class="csl-right-inline"><strong>Software review: COCI, the OpenCitations Index of Crossref open DOI-to-DOI citations</strong> <div class="csl-block">Ivan Heibi, Silvio Peroni, David Shotton</div> <em>Scientometrics</em> (2019-09-14) <a href="https://doi.org/ggzz8b">https://doi.org/ggzz8b</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1007/s11192-019-03217-6">10.1007/s11192-019-03217-6</a></div></div>
</div>
<div id="ref-1CjuavV0Y" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">103. </div><div class="csl-right-inline"><strong>Promise and Pitfalls of Extending Google's PageRank Algorithm to Citation Networks</strong> <div class="csl-block">S Maslov, S Redner</div> <em>Journal of Neuroscience</em> (2008-10-29) <a href="https://doi.org/fsfh8w">https://doi.org/fsfh8w</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1523/jneurosci.0002-08.2008">10.1523/jneurosci.0002-08.2008</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/18971452">18971452</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6671494">PMC6671494</a></div></div>
</div>
<div id="ref-DupOCXrp" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">104. </div><div class="csl-right-inline"><strong>Standardizing the Evaluation of Scientific and Academic Performance in Neurosurgery—Critical Review of the “h” Index and its Variants</strong> <div class="csl-block">Salah G Aoun, Bernard R Bendok, Rudy J Rahme, Ralph G Dacey Jr., HHunt Batjer</div> <em>World Neurosurgery</em> (2013-11) <a href="https://doi.org/fxtz98">https://doi.org/fxtz98</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.wneu.2012.01.052">10.1016/j.wneu.2012.01.052</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/22381859">22381859</a></div></div>
</div>
<div id="ref-Qd54lMyE" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">105. </div><div class="csl-right-inline"><strong>UNDERSTANDING THE LIMITATIONS OF THE JOURNAL IMPACT FACTOR</strong> <div class="csl-block">ANDREW P KURMIS</div> <em>The Journal of Bone and Joint Surgery-American Volume</em> (2003-12) <a href="https://doi.org/gh6fph">https://doi.org/gh6fph</a> <div class="csl-block">DOI: <a href="https://doi.org/10.2106/00004623-200312000-00028">10.2106/00004623-200312000-00028</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/14668520">14668520</a></div></div>
</div>
<div id="ref-o78W9pGh" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">106. </div><div class="csl-right-inline"><strong>A bird’s-eye view of deep learning in bioimage analysis</strong> <div class="csl-block">Erik Meijering</div> <em>Computational and Structural Biotechnology Journal</em> (2020) <a href="https://doi.org/gk5mtd">https://doi.org/gk5mtd</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1016/j.csbj.2020.08.003">10.1016/j.csbj.2020.08.003</a> · PMID: <a href="https://www.ncbi.nlm.nih.gov/pubmed/32994890">32994890</a> · PMCID: <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7494605">PMC7494605</a></div></div>
</div>
<div id="ref-lt4BNUoG" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">107. </div><div class="csl-right-inline"><strong>ImageNet: A large-scale hierarchical image database</strong> <div class="csl-block">Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei</div> <em>2009 IEEE Conference on Computer Vision and Pattern Recognition</em> (2009-06) <a href="https://doi.org/cvc7xp">https://doi.org/cvc7xp</a> <div class="csl-block">DOI: <a href="https://doi.org/10.1109/cvpr.2009.5206848">10.1109/cvpr.2009.5206848</a></div></div>
</div>
<div id="ref-XVMFUrSt" class="csl-entry" role="doc-biblioentry">
<div class="csl-left-margin">108. </div><div class="csl-right-inline"><strong>Training Compute-Optimal Large Language Models</strong> <div class="csl-block">Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, … Laurent Sifre</div> <em>arXiv</em> (2022-03-30) <a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a></div>
</div>
</div>
<!-- default theme -->

<style>
  /* import google fonts */
  @import url("https://fonts.googleapis.com/css?family=Open+Sans:400,600,700");
  @import url("https://fonts.googleapis.com/css?family=Source+Code+Pro");

  /* -------------------------------------------------- */
  /* global */
  /* -------------------------------------------------- */

  /* all elements */
  * {
    /* force sans-serif font unless specified otherwise */
    font-family: "Open Sans", "Helvetica", sans-serif;

    /* prevent text inflation on some mobile browsers */
    -webkit-text-size-adjust: none !important;
    -moz-text-size-adjust: none !important;
    -o-text-size-adjust: none !important;
    text-size-adjust: none !important;
  }

  @media only screen {
    /* "page" element */
    body {
      position: relative;
      box-sizing: border-box;
      font-size: 12pt;
      line-height: 1.5;
      max-width: 8.5in;
      margin: 20px auto;
      padding: 40px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* "page" element */
    body {
      padding: 20px;
      margin: 0;
      border-radius: 0;
      border: none;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05) inset;
      background: none;
    }
  }

  /* -------------------------------------------------- */
  /* headings */
  /* -------------------------------------------------- */

  /* all headings */
  h1,
  h2,
  h3,
  h4,
  h5,
  h6 {
    margin: 20px 0;
    padding: 0;
    font-weight: bold;
  }

  /* biggest heading */
  h1 {
    margin: 40px 0;
    text-align: center;
  }

  /* second biggest heading */
  h2 {
    margin-top: 30px;
    padding-bottom: 5px;
    border-bottom: solid 1px #bdbdbd;
  }

  /* heading font sizes */
  h1 {
    font-size: 2em;
  }
  h2 {
    font-size: 1.5em;
  }
  h3 {
    font-size: 1.35em;
  }
  h4 {
    font-size: 1.25em;
  }
  h5 {
    font-size: 1.15em;
  }
  h6 {
    font-size: 1em;
  }

  /* -------------------------------------------------- */
  /* manuscript header */
  /* -------------------------------------------------- */

  /* manuscript title */
  header > h1 {
    margin: 0;
  }

  /* manuscript title caption text (ie "automatically generated on") */
  header + p {
    text-align: center;
    margin-top: 10px;
  }

  /* -------------------------------------------------- */
  /* text elements */
  /* -------------------------------------------------- */

  /* links */
  a {
    color: #2196f3;
    overflow-wrap: break-word;
  }

  /* superscripts and subscripts */
  sub,
  sup {
    /* prevent from affecting line height */
    line-height: 0;
  }

  /* unordered and ordered lists*/
  ul,
  ol {
    padding-left: 20px;
  }

  /* class for styling text semibold */
  .semibold {
    font-weight: 600;
  }

  /* class for styling elements horizontally left aligned */
  .left {
    display: block;
    text-align: left;
    margin-left: auto;
    margin-right: 0;
    justify-content: left;
  }

  /* class for styling elements horizontally centered */
  .center {
    display: block;
    text-align: center;
    margin-left: auto;
    margin-right: auto;
    justify-content: center;
  }

  /* class for styling elements horizontally right aligned */
  .right {
    display: block;
    text-align: right;
    margin-left: 0;
    margin-right: auto;
    justify-content: right;
  }

  /* -------------------------------------------------- */
  /* section elements */
  /* -------------------------------------------------- */

  /* horizontal divider line */
  hr {
    border: none;
    height: 1px;
    background: #bdbdbd;
  }

  /* paragraphs, horizontal dividers, figures, tables, code */
  p,
  hr,
  figure,
  table,
  pre {
    /* treat all as "paragraphs", with consistent vertical margins */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* figures */
  /* -------------------------------------------------- */

  /* figure */
  figure {
    max-width: 100%;
    margin-left: auto;
    margin-right: auto;
  }

  /* figure caption */
  figcaption {
    padding: 0;
    padding-top: 10px;
  }

  /* figure image element */
  figure > img,
  figure > svg {
    max-width: 100%;
    display: block;
    margin-left: auto;
    margin-right: auto;
  }

  /* figure auto-number */
  img + figcaption > span:first-of-type,
  svg + figcaption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* tables */
  /* -------------------------------------------------- */

  /* table */
  table {
    border-collapse: collapse;
    border-spacing: 0;
    width: 100%;
    margin-left: auto;
    margin-right: auto;
  }

  /* table cells */
  th,
  td {
    border: solid 1px #bdbdbd;
    padding: 10px;
    /* squash table if too wide for page by forcing line breaks */
    overflow-wrap: break-word;
    word-break: break-word;
  }

  /* header row and even rows */
  th,
  tr:nth-child(2n) {
    background-color: #fafafa;
  }

  /* odd rows */
  tr:nth-child(2n + 1) {
    background-color: #ffffff;
  }

  /* table caption */
  caption {
    text-align: left;
    padding: 0;
    padding-bottom: 10px;
  }

  /* table auto-number */
  table > caption > span:first-of-type {
    font-weight: bold;
    margin-right: 5px;
  }

  /* -------------------------------------------------- */
  /* code */
  /* -------------------------------------------------- */

  /* multi-line code block */
  pre {
    padding: 10px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
    break-inside: avoid;
    text-align: left;
  }

  /* inline code, ie code within normal text */
  :not(pre) > code {
    padding: 0 4px;
    background-color: #eeeeee;
    color: #000000;
    border-radius: 5px;
  }

  /* code text */
  /* apply all children, to reach syntax highlighting sub-elements */
  code,
  code * {
    /* force monospace font */
    font-family: "Source Code Pro", "Courier New", monospace;
  }

  /* -------------------------------------------------- */
  /* quotes */
  /* -------------------------------------------------- */

  /* quoted text */
  blockquote {
    margin: 0;
    padding: 0;
    border-left: 4px solid #bdbdbd;
    padding-left: 16px;
    break-inside: avoid;
  }

  /* -------------------------------------------------- */
  /* banners */
  /* -------------------------------------------------- */

  /* info banners */
  .banner {
    box-sizing: border-box;
    display: block;
    position: relative;
    width: 100%;
    margin-top: 20px;
    margin-bottom: 20px;
    padding: 20px;
    text-align: center;
  }

  /* paragraph in banner */
  .banner > p {
    margin: 0;
  }

  /* -------------------------------------------------- */
  /* highlight colors */
  /* -------------------------------------------------- */

  .white {
    background: #ffffff;
  }
  .lightgrey {
    background: #eeeeee;
  }
  .grey {
    background: #757575;
  }
  .darkgrey {
    background: #424242;
  }
  .black {
    background: #000000;
  }
  .lightred {
    background: #ffcdd2;
  }
  .lightyellow {
    background: #ffecb3;
  }
  .lightgreen {
    background: #dcedc8;
  }
  .lightblue {
    background: #e3f2fd;
  }
  .lightpurple {
    background: #f3e5f5;
  }
  .red {
    background: #f44336;
  }
  .orange {
    background: #ff9800;
  }
  .yellow {
    background: #ffeb3b;
  }
  .green {
    background: #4caf50;
  }
  .blue {
    background: #2196f3;
  }
  .purple {
    background: #9c27b0;
  }
  .white,
  .lightgrey,
  .lightred,
  .lightyellow,
  .lightgreen,
  .lightblue,
  .lightpurple,
  .orange,
  .yellow,
  .white a,
  .lightgrey a,
  .lightred a,
  .lightyellow a,
  .lightgreen a,
  .lightblue a,
  .lightpurple a,
  .orange a,
  .yellow a {
    color: #000000;
  }
  .grey,
  .darkgrey,
  .black,
  .red,
  .green,
  .blue,
  .purple,
  .grey a,
  .darkgrey a,
  .black a,
  .red a,
  .green a,
  .blue a,
  .purple a {
    color: #ffffff;
  }

  /* -------------------------------------------------- */
  /* buttons */
  /* -------------------------------------------------- */

  /* class for styling links like buttons */
  .button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    margin: 5px;
    padding: 10px 20px;
    font-size: 0.75em;
    font-weight: 600;
    text-transform: uppercase;
    text-decoration: none;
    letter-spacing: 1px;
    background: none;
    color: #2196f3;
    border: solid 1px #bdbdbd;
    border-radius: 5px;
  }

  /* buttons when hovered */
  .button:hover:not([disabled]),
  .icon_button:hover:not([disabled]) {
    cursor: pointer;
    background: #f5f5f5;
  }

  /* buttons when disabled */
  .button[disabled],
  .icon_button[disabled] {
    opacity: 0.35;
    pointer-events: none;
  }

  /* class for styling buttons containg only single icon */
  .icon_button {
    display: inline-flex;
    justify-content: center;
    align-items: center;
    text-decoration: none;
    margin: 0;
    padding: 0;
    background: none;
    border-radius: 5px;
    border: none;
    width: 20px;
    height: 20px;
    min-width: 20px;
    min-height: 20px;
  }

  /* icon button inner svg image */
  .icon_button > svg {
    height: 16px;
  }

  /* -------------------------------------------------- */
  /* icons */
  /* -------------------------------------------------- */

  /* class for styling icons inline with text */
  .inline_icon {
    height: 1em;
    position: relative;
    top: 0.125em;
  }

  /* -------------------------------------------------- */
  /* references */
  /* -------------------------------------------------- */

  .csl-entry {
    margin-top: 15px;
    margin-bottom: 15px;
  }

  /* -------------------------------------------------- */
  /* print control */
  /* -------------------------------------------------- */

  @media print {
    @page {
      /* suggested printing margin */
      margin: 0.5in;
    }

    /* document and "page" elements */
    html,
    body {
      margin: 0;
      padding: 0;
      width: 100%;
      height: 100%;
    }

    /* "page" element */
    body {
      font-size: 11pt !important;
      line-height: 1.35;
    }

    /* all headings */
    h1,
    h2,
    h3,
    h4,
    h5,
    h6 {
      margin: 15px 0;
    }

    /* figures and tables */
    figure,
    table {
      font-size: 0.85em;
    }

    /* table cells */
    th,
    td {
      padding: 5px;
    }

    /* shrink font awesome icons */
    i.fas,
    i.fab,
    i.far,
    i.fal {
      transform: scale(0.85);
    }

    /* decrease banner margins */
    .banner {
      margin-top: 15px;
      margin-bottom: 15px;
      padding: 15px;
    }

    /* class for centering an element vertically on its own page */
    .page_center {
      margin: auto;
      width: 100%;
      height: 100%;
      display: flex;
      align-items: center;
      vertical-align: middle;
      break-before: page;
      break-after: page;
    }

    /* always insert a page break before the element */
    .page_break_before {
      break-before: page;
    }

    /* always insert a page break after the element */
    .page_break_after {
      break-after: page;
    }

    /* avoid page break before the element */
    .page_break_before_avoid {
      break-before: avoid;
    }

    /* avoid page break after the element */
    .page_break_after_avoid {
      break-after: avoid;
    }

    /* avoid page break inside the element */
    .page_break_inside_avoid {
      break-inside: avoid;
    }
  }

  /* -------------------------------------------------- */
  /* override pandoc css quirks */
  /* -------------------------------------------------- */

  .sourceCode {
    /* prevent unsightly overflow in wide code blocks */
    overflow: auto !important;
  }

  div.sourceCode {
    /* prevent background fill on top-most code block  container */
    background: none !important;
  }

  .sourceCode * {
    /* force consistent line spacing */
    line-height: 1.5 !important;
  }

  div.sourceCode {
    /* style code block margins same as <pre> element */
    margin-top: 20px;
    margin-bottom: 20px;
  }

  /* -------------------------------------------------- */
  /* tablenos */
  /* -------------------------------------------------- */

  /* tablenos wrapper */
  .tablenos {
    width: 100%;
    margin: 20px 0;
  }

  .tablenos > table {
    /* move margins from table to table_wrapper to allow margin collapsing */
    margin: 0;
  }

  @media only screen {
    /* tablenos wrapper */
    .tablenos {
      /* show scrollbar on tables if necessary to prevent overflow */
      overflow-x: auto !important;
    }

    .tablenos th,
    .tablenos td {
      overflow-wrap: unset !important;
      word-break: unset !important;
    }

    /* table in wrapper */
    .tablenos table,
    .tablenos table * {
      /* don't break table words */
      overflow-wrap: normal !important;
    }
  }
</style>
<!-- 
    Plugin Core

    Functions needed for and shared across all first-party plugins.
-->

<script>
  // get element that is target of hash (from link element or url)
  function getHashTarget(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector(`[id="${id}"]`);
    if (!target) return;

    // if figure or table, modify target to get expected element
    if (id.indexOf("fig:") === 0) target = target.querySelector("figure");
    if (id.indexOf("tbl:") === 0) target = target.querySelector("table");

    return target;
  }

  // get position/dimensions of element or viewport
  function getRectInView(element) {
    let rect = {};
    rect.left = 0;
    rect.top = 0;
    rect.right = document.documentElement.clientWidth;
    rect.bottom = document.documentElement.clientHeight;
    let style = {};

    if (element instanceof HTMLElement) {
      rect = element.getBoundingClientRect();
      style = window.getComputedStyle(element);
    }

    const margin = {};
    margin.left = parseFloat(style.marginLeftWidth) || 0;
    margin.top = parseFloat(style.marginTopWidth) || 0;
    margin.right = parseFloat(style.marginRightWidth) || 0;
    margin.bottom = parseFloat(style.marginBottomWidth) || 0;

    const border = {};
    border.left = parseFloat(style.borderLeftWidth) || 0;
    border.top = parseFloat(style.borderTopWidth) || 0;
    border.right = parseFloat(style.borderRightWidth) || 0;
    border.bottom = parseFloat(style.borderBottomWidth) || 0;

    const newRect = {};
    newRect.left = rect.left + margin.left + border.left;
    newRect.top = rect.top + margin.top + border.top;
    newRect.right = rect.right + margin.right + border.right;
    newRect.bottom = rect.bottom + margin.bottom + border.bottom;
    newRect.width = newRect.right - newRect.left;
    newRect.height = newRect.bottom - newRect.top;

    return newRect;
  }

  // get position of element relative to page
  function getRectInPage(element) {
    const rect = getRectInView(element);
    const body = getRectInView(document.body);

    const newRect = {};
    newRect.left = rect.left - body.left;
    newRect.top = rect.top - body.top;
    newRect.right = rect.right - body.left;
    newRect.bottom = rect.bottom - body.top;
    newRect.width = rect.width;
    newRect.height = rect.height;

    return newRect;
  }

  // get closest element before specified element that matches query
  function firstBefore(element, query) {
    while (element && element !== document.body && !element.matches(query))
      element = element.previousElementSibling || element.parentNode;

    return element;
  }

  // check if element is part of collapsed heading
  function isCollapsed(element) {
    while (element && element !== document.body) {
      if (element.dataset.collapsed === "true") return true;
      element = element.parentNode;
    }
    return false;
  }

  // expand any collapsed parent containers of element if necessary
  function expandElement(element) {
    if (isCollapsed(element)) {
      // accordion plugin
      const heading = firstBefore(element, "h2");
      if (heading) heading.click();
      // details/summary HTML element
      const summary = firstBefore(element, "summary");
      if (summary) summary.click();
    }
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);

    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // get list of elements after a start element up to element matching query
  function nextUntil(element, query, exclude) {
    const elements = [];
    while (((element = element.nextElementSibling), element)) {
      if (element.matches(query)) break;
      if (!element.matches(exclude)) elements.push(element);
    }
    return elements;
  }
</script>
<!--
  Accordion Plugin

  Allows sections of content under h2 headings to be collapsible.
-->

<script type="module">
  // whether to always start expanded ('false'), always start collapsed
  // ('true'), or start collapsed when screen small ('auto')
  const startCollapsed = "auto";

  // start script
  function start() {
    // run through each <h2> heading
    const headings = document.querySelectorAll("h2");
    for (const heading of headings) {
      addArrow(heading);

      // start expanded/collapsed based on option
      if (
        startCollapsed === "true" ||
        (startCollapsed === "auto" && isSmallScreen()) ||
        heading.dataset.collapsed === "true"
      )
        collapseHeading(heading);
      else expandElement(heading);
    }

    // attach hash change listener to window
    window.addEventListener("hashchange", onHashChange);
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) goToElement(target);
  }

  // add arrow to heading
  function addArrow(heading) {
    // add arrow button
    const arrow = document.createElement("button");
    arrow.innerHTML = document.querySelector(".icon_angle_down").innerHTML;
    arrow.classList.add("icon_button", "accordion_arrow");
    heading.insertBefore(arrow, heading.firstChild);

    // attach click listener to heading and button
    heading.addEventListener("click", onHeadingClick);
    arrow.addEventListener("click", onArrowClick);
  }

  // determine if on mobile-like device with small screen
  function isSmallScreen() {
    return Math.min(window.innerWidth, window.innerHeight) < 480;
  }

  // when <h2> heading is clicked
  function onHeadingClick(event) {
    // only collapse if <h2> itself is target of click (eg, user did
    // not click on anchor within <h2>)
    if (event.target === this) toggleCollapse(this);
  }

  // when arrow button is clicked
  function onArrowClick() {
    toggleCollapse(this.parentNode);
  }

  // collapse section if expanded, expand if collapsed
  function toggleCollapse(heading) {
    if (heading.dataset.collapsed === "false") collapseHeading(heading);
    else expandElement(heading);
  }

  // elements to exclude from collapse, such as table of contents panel,
  // hypothesis panel, etc
  const exclude = "#toc_panel, div.annotator-frame, #lightbox_overlay";

  // collapse section
  function collapseHeading(heading) {
    heading.setAttribute("data-collapsed", "true");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "true");
  }

  // expand section
  function expandElement(heading) {
    heading.setAttribute("data-collapsed", "false");
    const children = getChildren(heading);
    for (const child of children) child.setAttribute("data-collapsed", "false");
  }

  // get list of elements between this <h2> and next <h2> or <h1>
  // ("children" of the <h2> section)
  function getChildren(heading) {
    return nextUntil(heading, "h2, h1", exclude);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle down icon -->

<template class="icon_angle_down">
  <!-- modified from: https://fontawesome.com/icons/angle-down -->
  <svg width="16" height="16" viewBox="0 0 448 512">
    <path
      fill="currentColor"
      d="M207.029 381.476L12.686 187.132c-9.373-9.373-9.373-24.569 0-33.941l22.667-22.667c9.357-9.357 24.522-9.375 33.901-.04L224 284.505l154.745-154.021c9.379-9.335 24.544-9.317 33.901.04l22.667 22.667c9.373 9.373 9.373 24.569 0 33.941L240.971 381.476c-9.373 9.372-24.569 9.372-33.942 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* accordion arrow button */
    .accordion_arrow {
      margin-right: 10px;
    }

    /* arrow icon when <h2> data-collapsed attribute true */
    h2[data-collapsed="true"] > .accordion_arrow > svg {
      transform: rotate(-90deg);
    }

    /* all elements (except <h2>'s) when data-collapsed attribute true */
    *:not(h2)[data-collapsed="true"] {
      display: none;
    }

    /* accordion arrow button when hovered and <h2>'s when hovered */
    .accordion_arrow:hover,
    h2[data-collapsed="true"]:hover,
    h2[data-collapsed="false"]:hover {
      cursor: pointer;
    }
  }

  /* always hide accordion arrow button on print */
  @media only print {
    .accordion_arrow {
      display: none;
    }
  }
</style>
<!--
  Anchors Plugin

  Adds an anchor next to each of a certain type of element that provides a
  human-readable url to that specific item/position in the document (e.g.
  "manuscript.html#abstract"). It also makes it such that scrolling out of view
  of a target removes its identifier from the url.
-->

<script type="module">
  // which types of elements to add anchors next to, in "document.querySelector"
  // format
  const typesQuery =
    'h1, h2, h3, div[id^="fig:"], div[id^="tbl:"], span[id^="eq:"]';

  // start script
  function start() {
    // add anchor to each element of specified types
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) addAnchor(element);

    // attach scroll listener to window
    window.addEventListener("scroll", onScroll);
  }

  // when window is scrolled
  function onScroll() {
    // if url has hash and user has scrolled out of view of hash
    // target, remove hash from url
    const tolerance = 100;
    const target = getHashTarget();
    if (target) {
      if (
        target.getBoundingClientRect().top > window.innerHeight + tolerance ||
        target.getBoundingClientRect().bottom < 0 - tolerance
      )
        history.pushState(null, null, " ");
    }
  }

  // add anchor to element
  function addAnchor(element) {
    let addTo; // element to add anchor button to

    // if figure or table, modify withId and addTo to get expected
    // elements
    if (element.id.indexOf("fig:") === 0) {
      addTo = element.querySelector("figcaption");
    } else if (element.id.indexOf("tbl:") === 0) {
      addTo = element.querySelector("caption");
    } else if (element.id.indexOf("eq:") === 0) {
      addTo = element.querySelector(".eqnos-number");
    }

    addTo = addTo || element;
    const id = element.id || null;

    // do not add anchor if element doesn't have assigned id.
    // id is generated by pandoc and is assumed to be unique and
    // human-readable
    if (!id) return;

    // create anchor button
    const anchor = document.createElement("a");
    anchor.innerHTML = document.querySelector(".icon_link").innerHTML;
    anchor.title = "Link to this part of the document";
    anchor.classList.add("icon_button", "anchor");
    anchor.dataset.ignore = "true";
    anchor.href = "#" + id;
    addTo.appendChild(anchor);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- link icon -->

<template class="icon_link">
  <!-- modified from: https://fontawesome.com/icons/link -->
  <svg width="16" height="16" viewBox="0 0 512 512">
    <path
      fill="currentColor"
      d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* anchor button */
    .anchor {
      opacity: 0;
      margin-left: 5px;
    }

    /* anchor buttons within <h2>'s */
    h2 .anchor {
      margin-left: 10px;
    }

    /* anchor buttons when hovered/focused and anything containing an anchor button when hovered */
    *:hover > .anchor,
    .anchor:hover,
    .anchor:focus {
      opacity: 1;
    }

    /* anchor button when hovered */
    .anchor:hover {
      cursor: pointer;
    }
  }

  /* always show anchor button on devices with no mouse/hover ability */
  @media (hover: none) {
    .anchor {
      opacity: 1;
    }
  }

  /* always hide anchor button on print */
  @media only print {
    .anchor {
      display: none;
    }
  }
</style>
<!-- 
    Attributes Plugin

    Allows arbitrary HTML attributes to be attached to (almost) any element.
    Place an HTML comment inside or next to the desired element with the content:
    $attribute="value"
-->

<script type="module">
  // start script
  function start() {
    // get list of comments in document
    const comments = findComments();

    for (const comment of comments)
      if (comment.parentElement)
        addAttributes(comment.parentElement, comment.nodeValue.trim());
  }

  // add html attributes to specified element based on string of
  // html attributes and values
  function addAttributes(element, text) {
    // regex's for finding attribute/value pairs in the format of
    // attribute="value" or attribute='value
    const regex2 = /\$([a-zA-Z\-]+)?=\"(.+?)\"/;
    const regex1 = /\$([a-zA-Z\-]+)?=\'(.+?)\'/;

    // loop through attribute/value pairs
    let match;
    while ((match = text.match(regex2) || text.match(regex1))) {
      // get attribute and value from regex capture groups
      let attribute = match[1];
      let value = match[2];

      // remove from string
      text = text.substring(match.index + match[0].length);

      if (!attribute || !value) break;

      // set attribute of parent element
      try {
        element.setAttribute(attribute, value);
      } catch (error) {
        console.log(error);
      }

      // special case for colspan
      if (attribute === "colspan") removeTableCells(element, value);
    }
  }

  // get list of comment elements in document
  function findComments() {
    const comments = [];

    // iterate over comment nodes in document
    function acceptNode(node) {
      return NodeFilter.FILTER_ACCEPT;
    }
    const iterator = document.createNodeIterator(
      document.body,
      NodeFilter.SHOW_COMMENT,
      acceptNode
    );
    let node;
    while ((node = iterator.nextNode())) comments.push(node);

    return comments;
  }

  // remove certain number of cells after specified cell
  function removeTableCells(cell, number) {
    number = parseInt(number);
    if (!number) return;

    // remove elements
    for (; number > 1; number--) {
      if (cell.nextElementSibling) cell.nextElementSibling.remove();
    }
  }

  // start script on DOMContentLoaded instead of load to ensure this plugins
  // runs before other plugins
  window.addEventListener("DOMContentLoaded", start);
</script>
<!--
  Jump to First Plugin

  Adds a button next to each reference entry, figure, and table that jumps the
  page to the first occurrence of a link to that item in the manuscript.
-->

<script type="module">
  // whether to add buttons next to reference entries
  const references = "true";
  // whether to add buttons next to figures
  const figures = "true";
  // whether to add buttons next to tables
  const tables = "true";

  // start script
  function start() {
    if (references !== "false")
      makeButtons(`div[id^="ref-"]`, ".csl-left-margin", "reference");
    if (figures !== "false")
      makeButtons(`div[id^="fig:"]`, "figcaption", "figure");
    if (tables !== "false") makeButtons(`div[id^="tbl:"]`, "caption", "table");
  }

  // when jump button clicked
  function onButtonClick() {
    const first = getFirstOccurrence(this.dataset.id);
    if (!first) return;

    // update url hash so navigating "back" in history will return user to button
    window.location.hash = this.dataset.id;
    // scroll to link
    const timeout = function () {
      goToElement(first, window.innerHeight * 0.5);
    };
    window.setTimeout(timeout, 0);
  }

  // get first occurrence of link to item in document
  function getFirstOccurrence(id) {
    let query = "a";
    query += '[href="#' + id + '"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelector(query);
  }

  // add button next to each reference entry, figure, or table
  function makeButtons(query, containerQuery, subject) {
    const elements = document.querySelectorAll(query);
    for (const element of elements) {
      const id = element.id;
      const buttonContainer = element.querySelector(containerQuery);
      const first = getFirstOccurrence(id);

      // if can't find link to reference or place to put button, ignore
      if (!first || !buttonContainer) continue;

      // make jump button
      let button = document.createElement("button");
      button.classList.add("icon_button", "jump_arrow");
      button.title = `Jump to the first occurrence of this ${subject} in the document`;
      const icon = document.querySelector(".icon_angle_double_up");
      button.innerHTML = icon.innerHTML;
      button.dataset.id = id;
      button.dataset.ignore = "true";
      button.addEventListener("click", onButtonClick);
      buttonContainer.prepend(button);
    }
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- angle double up icon -->

<template class="icon_angle_double_up">
  <!-- modified from: https://fontawesome.com/icons/angle-double-up -->
  <svg width="16" height="16" viewBox="0 0 320 512">
    <path
      fill="currentColor"
      d="M177 255.7l136 136c9.4 9.4 9.4 24.6 0 33.9l-22.6 22.6c-9.4 9.4-24.6 9.4-33.9 0L160 351.9l-96.4 96.4c-9.4 9.4-24.6 9.4-33.9 0L7 425.7c-9.4-9.4-9.4-24.6 0-33.9l136-136c9.4-9.5 24.6-9.5 34-.1zm-34-192L7 199.7c-9.4 9.4-9.4 24.6 0 33.9l22.6 22.6c9.4 9.4 24.6 9.4 33.9 0l96.4-96.4 96.4 96.4c9.4 9.4 24.6 9.4 33.9 0l22.6-22.6c9.4-9.4 9.4-24.6 0-33.9l-136-136c-9.2-9.4-24.4-9.4-33.8 0z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* jump button */
    .jump_arrow {
      position: relative;
      top: 0.125em;
      margin-right: 5px;
    }
  }

  /* always hide jump button on print */
  @media only print {
    .jump_arrow {
      display: none;
    }
  }
</style>
<!-- 
    Lightbox Plugin

    Makes it such that when a user clicks on an image, the image fills the
    screen and the user can pan/drag/zoom the image and navigate between other
    images in the document.
-->

<script type="module">
  // list of possible zoom/scale factors
  const zooms =
    "0.1, 0.25, 0.333333, 0.5, 0.666666, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.5, 3, 3.5, 4, 5, 6, 7, 8";
  // whether to fit image to view ('fit'), display at 100% and shrink if
  // necessary ('shrink'), or always display at 100% ('100')
  const defaultZoom = "fit";
  // whether to zoom in/out toward center of view ('true') or mouse ('false')
  const centerZoom = "false";

  // start script
  function start() {
    // run through each <img> element
    const imgs = document.querySelectorAll("figure > img");
    let count = 1;
    for (const img of imgs) {
      img.classList.add("lightbox_document_img");
      img.dataset.number = count;
      img.dataset.total = imgs.length;
      img.addEventListener("click", openLightbox);
      count++;
    }

    // attach mouse and key listeners to window
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("keyup", onKeyUp);
  }

  // when mouse is moved anywhere in window
  function onWindowMouseMove(event) {
    window.mouseX = event.clientX;
    window.mouseY = event.clientY;
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("lightbox_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("lightbox_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeLightbox();
        break;
    }
  }

  // open lightbox
  function openLightbox() {
    const lightbox = makeLightbox(this);
    if (!lightbox) return;

    blurBody(lightbox);
    document.body.appendChild(lightbox);
  }

  // make lightbox
  function makeLightbox(img) {
    // delete lightbox if it exists, start fresh
    closeLightbox();

    // create screen overlay containing lightbox
    const overlay = document.createElement("div");
    overlay.id = "lightbox_overlay";

    // create image info boxes
    const numberInfo = document.createElement("div");
    const zoomInfo = document.createElement("div");
    numberInfo.id = "lightbox_number_info";
    zoomInfo.id = "lightbox_zoom_info";

    // create container for image
    const imageContainer = document.createElement("div");
    imageContainer.id = "lightbox_image_container";
    const lightboxImg = makeLightboxImg(
      img,
      imageContainer,
      numberInfo,
      zoomInfo
    );
    imageContainer.appendChild(lightboxImg);

    // create bottom container for caption and navigation buttons
    const bottomContainer = document.createElement("div");
    bottomContainer.id = "lightbox_bottom_container";
    const caption = makeCaption(img);
    const prevButton = makePrevButton(img);
    const nextButton = makeNextButton(img);
    bottomContainer.appendChild(prevButton);
    bottomContainer.appendChild(caption);
    bottomContainer.appendChild(nextButton);

    // attach top middle and bottom to overlay
    overlay.appendChild(numberInfo);
    overlay.appendChild(zoomInfo);
    overlay.appendChild(imageContainer);
    overlay.appendChild(bottomContainer);

    return overlay;
  }

  // make <img> object that is intuitively draggable and zoomable
  function makeLightboxImg(sourceImg, container, numberInfoBox, zoomInfoBox) {
    // create copy of source <img>
    const img = sourceImg.cloneNode(true);
    img.classList.remove("lightbox_document_img");
    img.removeAttribute("id");
    img.removeAttribute("width");
    img.removeAttribute("height");
    img.style.position = "unset";
    img.style.margin = "0";
    img.style.padding = "0";
    img.style.width = "";
    img.style.height = "";
    img.style.minWidth = "";
    img.style.minHeight = "";
    img.style.maxWidth = "";
    img.style.maxHeight = "";
    img.id = "lightbox_img";

    // build sorted list of zoomSteps
    const zoomSteps = zooms.split(/[^0-9.]/).map((step) => parseFloat(step));
    zoomSteps.sort((a, b) => a - b);

    // <img> object property variables
    let zoom = 1;
    let translateX = 0;
    let translateY = 0;
    let clickMouseX = undefined;
    let clickMouseY = undefined;
    let clickTranslateX = undefined;
    let clickTranslateY = undefined;

    updateNumberInfo();

    // update image numbers displayed in info box
    function updateNumberInfo() {
      numberInfoBox.innerHTML =
        sourceImg.dataset.number + " of " + sourceImg.dataset.total;
    }

    // update zoom displayed in info box
    function updateZoomInfo() {
      let zoomInfo = zoom * 100;
      if (!Number.isInteger(zoomInfo)) zoomInfo = zoomInfo.toFixed(2);
      zoomInfoBox.innerHTML = zoomInfo + "%";
    }

    // move to closest zoom step above current zoom
    const zoomIn = function () {
      for (const zoomStep of zoomSteps) {
        if (zoomStep > zoom) {
          zoom = zoomStep;
          break;
        }
      }
      updateTransform();
    };

    // move to closest zoom step above current zoom
    const zoomOut = function () {
      zoomSteps.reverse();
      for (const zoomStep of zoomSteps) {
        if (zoomStep < zoom) {
          zoom = zoomStep;
          break;
        }
      }
      zoomSteps.reverse();

      updateTransform();
    };

    // update display of <img> based on scale/translate properties
    const updateTransform = function () {
      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      // get new width/height after scale
      const rect = img.getBoundingClientRect();
      // limit translate
      translateX = Math.max(translateX, -rect.width / 2);
      translateX = Math.min(translateX, rect.width / 2);
      translateY = Math.max(translateY, -rect.height / 2);
      translateY = Math.min(translateY, rect.height / 2);

      // set transform
      img.style.transform =
        "translate(" +
        (translateX || 0) +
        "px," +
        (translateY || 0) +
        "px) scale(" +
        (zoom || 1) +
        ")";

      updateZoomInfo();
    };

    // fit <img> to container
    const fit = function () {
      // no x/y offset, 100% zoom by default
      translateX = 0;
      translateY = 0;
      zoom = 1;

      // widths of <img> and container
      const imgWidth = img.naturalWidth;
      const imgHeight = img.naturalHeight;
      const containerWidth = parseFloat(
        window.getComputedStyle(container).width
      );
      const containerHeight = parseFloat(
        window.getComputedStyle(container).height
      );

      // how much zooming is needed to fit <img> to container
      const xRatio = imgWidth / containerWidth;
      const yRatio = imgHeight / containerHeight;
      const maxRatio = Math.max(xRatio, yRatio);
      const newZoom = 1 / maxRatio;

      // fit <img> to container according to option
      if (defaultZoom === "shrink") {
        if (maxRatio > 1) zoom = newZoom;
      } else if (defaultZoom === "fit") zoom = newZoom;

      updateTransform();
    };

    // when mouse wheel is rolled anywhere in container
    const onContainerWheel = function (event) {
      if (!event) return;

      // let ctrl + mouse wheel to zoom behave as normal
      if (event.ctrlKey) return;

      // prevent normal scroll behavior
      event.preventDefault();
      event.stopPropagation();

      // point around which to scale img
      const viewRect = container.getBoundingClientRect();
      const viewX = (viewRect.left + viewRect.right) / 2;
      const viewY = (viewRect.top + viewRect.bottom) / 2;
      const originX = centerZoom === "true" ? viewX : mouseX;
      const originY = centerZoom === "true" ? viewY : mouseY;

      // get point on image under origin
      const oldRect = img.getBoundingClientRect();
      const oldPercentX = (originX - oldRect.left) / oldRect.width;
      const oldPercentY = (originY - oldRect.top) / oldRect.height;

      // increment/decrement zoom
      if (event.deltaY < 0) zoomIn();
      if (event.deltaY > 0) zoomOut();

      // get offset between previous image point and origin
      const newRect = img.getBoundingClientRect();
      const offsetX = originX - (newRect.left + newRect.width * oldPercentX);
      const offsetY = originY - (newRect.top + newRect.height * oldPercentY);

      // translate image to keep image point under origin
      translateX += offsetX;
      translateY += offsetY;

      // perform translate
      updateTransform();
    };

    // when container is clicked
    function onContainerClick(event) {
      // if container itself is target of click, and not other
      // element above it
      if (event.target === this) closeLightbox();
    }

    // when mouse button is pressed on image
    const onImageMouseDown = function (event) {
      // store original mouse position relative to image
      clickMouseX = window.mouseX;
      clickMouseY = window.mouseY;
      clickTranslateX = translateX;
      clickTranslateY = translateY;
      event.stopPropagation();
      event.preventDefault();
    };

    // when mouse button is released anywhere in window
    const onWindowMouseUp = function (event) {
      // reset original mouse position
      clickMouseX = undefined;
      clickMouseY = undefined;
      clickTranslateX = undefined;
      clickTranslateY = undefined;

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mouseup", onWindowMouseUp);
    };

    // when mouse is moved anywhere in window
    const onWindowMouseMove = function (event) {
      if (
        clickMouseX === undefined ||
        clickMouseY === undefined ||
        clickTranslateX === undefined ||
        clickTranslateY === undefined
      )
        return;

      // offset image based on original and current mouse position
      translateX = clickTranslateX + window.mouseX - clickMouseX;
      translateY = clickTranslateY + window.mouseY - clickMouseY;
      updateTransform();
      event.preventDefault();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("mousemove", onWindowMouseMove);
    };

    // when window is resized
    const onWindowResize = function (event) {
      fit();

      // remove global listener if lightbox removed from document
      if (!document.body.contains(container))
        window.removeEventListener("resize", onWindowResize);
    };

    // attach the necessary event listeners
    img.addEventListener("dblclick", fit);
    img.addEventListener("mousedown", onImageMouseDown);
    container.addEventListener("wheel", onContainerWheel);
    container.addEventListener("mousedown", onContainerClick);
    container.addEventListener("touchstart", onContainerClick);
    window.addEventListener("mouseup", onWindowMouseUp);
    window.addEventListener("mousemove", onWindowMouseMove);
    window.addEventListener("resize", onWindowResize);

    // run fit() after lightbox atttached to document and <img> Loaded
    // so needed container and img dimensions available
    img.addEventListener("load", fit);

    return img;
  }

  // make caption
  function makeCaption(img) {
    const caption = document.createElement("div");
    caption.id = "lightbox_caption";
    const captionSource = img.nextElementSibling;
    if (captionSource.tagName.toLowerCase() === "figcaption") {
      const captionCopy = makeCopy(captionSource);
      caption.innerHTML = captionCopy.innerHTML;
    }

    caption.addEventListener("touchstart", function (event) {
      event.stopPropagation();
    });

    return caption;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // make button to jump to previous image in document
  function makePrevButton(img) {
    const prevButton = document.createElement("button");
    prevButton.id = "lightbox_prev_button";
    prevButton.title = "Jump to the previous image in the document [←]";
    prevButton.classList.add("icon_button", "lightbox_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;

    // attach click listeners to button
    prevButton.addEventListener("click", function () {
      getPrevImg(img).click();
    });

    return prevButton;
  }

  // make button to jump to next image in document
  function makeNextButton(img) {
    const nextButton = document.createElement("button");
    nextButton.id = "lightbox_next_button";
    nextButton.title = "Jump to the next image in the document [→]";
    nextButton.classList.add("icon_button", "lightbox_button");
    nextButton.innerHTML = document.querySelector(
      ".icon_caret_right"
    ).innerHTML;

    // attach click listeners to button
    nextButton.addEventListener("click", function () {
      getNextImg(img).click();
    });

    return nextButton;
  }

  // get previous image in document
  function getPrevImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if < 1
    if (index - 1 >= 0) index--;
    else index = imgs.length - 1;
    return imgs[index];
  }

  // get next image in document
  function getNextImg(img) {
    const imgs = document.querySelectorAll(".lightbox_document_img");

    // find index of provided img
    let index;
    for (index = 0; index < imgs.length; index++) {
      if (imgs[index] === img) break;
    }

    // wrap index to other side if > total
    if (index + 1 <= imgs.length - 1) index++;
    else index = 0;
    return imgs[index];
  }

  // close lightbox
  function closeLightbox() {
    focusBody();

    const lightbox = document.getElementById("lightbox_overlay");
    if (lightbox) lightbox.remove();
  }

  // make all elements behind lightbox non-focusable
  function blurBody(overlay) {
    const all = document.querySelectorAll("*");
    for (const element of all) element.tabIndex = -1;
    document.body.classList.add("body_no_scroll");
  }

  // make all elements focusable again
  function focusBody() {
    const all = document.querySelectorAll("*");
    for (const element of all) element.removeAttribute("tabIndex");
    document.body.classList.remove("body_no_scroll");
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* regular <img> in document when hovered */
    img.lightbox_document_img:hover {
      cursor: pointer;
    }

    .body_no_scroll {
      overflow: hidden !important;
    }

    /* screen overlay */
    #lightbox_overlay {
      display: flex;
      flex-direction: column;
      position: fixed;
      left: 0;
      top: 0;
      right: 0;
      bottom: 0;
      background: rgba(0, 0, 0, 0.75);
      z-index: 3;
    }

    /* middle area containing lightbox image */
    #lightbox_image_container {
      flex-grow: 1;
      display: flex;
      justify-content: center;
      align-items: center;
      overflow: hidden;
      position: relative;
      padding: 20px;
    }

    /* bottom area containing caption */
    #lightbox_bottom_container {
      display: flex;
      justify-content: center;
      align-items: center;
      height: 100px;
      min-height: 100px;
      max-height: 100px;
      background: rgba(0, 0, 0, 0.5);
    }

    /* image number info text box */
    #lightbox_number_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      left: 2px;
      top: 0;
      z-index: 4;
    }

    /* zoom info text box */
    #lightbox_zoom_info {
      position: absolute;
      color: #ffffff;
      font-weight: 600;
      right: 2px;
      top: 0;
      z-index: 4;
    }

    /* copy of image caption */
    #lightbox_caption {
      box-sizing: border-box;
      display: inline-block;
      width: 100%;
      max-height: 100%;
      padding: 10px 0;
      text-align: center;
      overflow-y: auto;
      color: #ffffff;
    }

    /* navigation previous/next button */
    .lightbox_button {
      width: 100px;
      height: 100%;
      min-width: 100px;
      min-height: 100%;
      color: #ffffff;
    }

    /* navigation previous/next button when hovered */
    .lightbox_button:hover {
      background: none !important;
    }

    /* navigation button icon */
    .lightbox_button > svg {
      height: 25px;
    }

    /* figure auto-number */
    #lightbox_caption > span:first-of-type {
      font-weight: bold;
      margin-right: 5px;
    }

    /* lightbox image when hovered */
    #lightbox_img:hover {
      cursor: grab;
    }

    /* lightbox image when grabbed */
    #lightbox_img:active {
      cursor: grabbing;
    }
  }

  /* when on screen < 480px wide */
  @media only screen and (max-width: 480px) {
    /* make navigation buttons skinnier on small screens to make more room for caption text */
    .lightbox_button {
      width: 50px;
      min-width: 50px;
    }
  }

  /* always hide lightbox on print */
  @media only print {
    #lightbox_overlay {
      display: none;
    }
  }
</style>
<!-- 
  Link Highlight Plugin

  Makes it such that when a user hovers or focuses a link, other links that have
  the same target will be highlighted. It also makes it such that when clicking
  a link, the target of the link (eg reference, figure, table) is briefly
  highlighted.
-->

<script type="module">
  // whether to also highlight links that go to external urls
  const externalLinks = "false";
  // whether user must click off to unhighlight instead of just
  // un-hovering
  const clickUnhighlight = "false";
  // whether to also highlight links that are unique
  const highlightUnique = "true";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach mouse and focus listeners to link
      link.addEventListener("mouseenter", onLinkFocus);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("mouseleave", onLinkUnhover);
    }

    // attach click and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("hashchange", onHashChange);

    // run hash change on window load in case user has navigated
    // directly to hash
    onHashChange();
  }

  // when link is focused (tabbed to) or hovered
  function onLinkFocus() {
    highlight(this);
  }

  // when link is unhovered
  function onLinkUnhover() {
    if (clickUnhighlight !== "true") unhighlightAll();
  }

  // when the mouse is clicked anywhere in window
  function onClick(event) {
    unhighlightAll();
  }

  // when hash (eg manuscript.html#introduction) changes
  function onHashChange() {
    const target = getHashTarget();
    if (target) glowElement(target);
  }

  // start glow sequence on an element
  function glowElement(element) {
    const startGlow = function () {
      onGlowEnd();
      element.dataset.glow = "true";
      element.addEventListener("animationend", onGlowEnd);
    };
    const onGlowEnd = function () {
      element.removeAttribute("data-glow");
      element.removeEventListener("animationend", onGlowEnd);
    };
    startGlow();
  }

  // highlight link and all others with same target
  function highlight(link) {
    // force unhighlight all to start fresh
    unhighlightAll();

    // get links with same target
    if (!link) return;
    const sameLinks = getSameLinks(link);

    // if link unique and option is off, exit and don't highlight
    if (sameLinks.length <= 1 && highlightUnique !== "true") return;

    // highlight all same links, and "select" (special highlight) this
    // one
    for (const sameLink of sameLinks) {
      if (sameLink === link) sameLink.setAttribute("data-selected", "true");
      else sameLink.setAttribute("data-highlighted", "true");
    }
  }

  // unhighlight all links
  function unhighlightAll() {
    const links = getLinks();
    for (const link of links) {
      link.setAttribute("data-selected", "false");
      link.setAttribute("data-highlighted", "false");
    }
  }

  // get links with same target
  function getSameLinks(link) {
    const results = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        results.push(otherLink);
    }
    return results;
  }

  // get all links of types we wish to handle
  function getLinks() {
    let query = "a";
    if (externalLinks !== "true") query += '[href^="#"]';
    // exclude buttons, anchor links, toc links, etc
    query += ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    return document.querySelectorAll(query);
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<style>
  @media only screen {
    /* anything with data-highlighted attribute true */
    [data-highlighted="true"] {
      background: #ffeb3b;
    }

    /* anything with data-selected attribute true */
    [data-selected="true"] {
      background: #ff8a65 !important;
    }

    /* animation definition for glow */
    @keyframes highlight_glow {
      0% {
        background: none;
      }
      10% {
        background: #bbdefb;
      }
      100% {
        background: none;
      }
    }

    /* anything with data-glow attribute true */
    [data-glow="true"] {
      animation: highlight_glow 2s;
    }
  }
</style>
<!--
  Table of Contents Plugin

  Provides a "table of contents" (toc) panel on the side of the document that
  allows the user to conveniently navigate between sections of the document.
-->

<script type="module">
  // which types of elements to add links for, in "document.querySelector" format
  const typesQuery = "h1, h2, h3";
  // whether toc starts open. use 'true' or 'false', or 'auto' to
  // use 'true' behavior when screen wide enough and 'false' when not
  const startOpen = "false";
  // whether toc closes when clicking on toc link. use 'true' or
  // 'false', or 'auto' to use 'false' behavior when screen wide
  // enough and 'true' when not
  const clickClose = "auto";
  // if list item is more than this many characters, text will be
  // truncated
  const charLimit = "50";
  // whether or not to show bullets next to each toc item
  const bullets = "false";

  // start script
  function start() {
    // make toc panel and populate with entries (links to document
    // sections)
    const panel = makePanel();
    if (!panel) return;
    makeEntries(panel);
    // attach panel to document after making entries, so 'toc' heading
    // in panel isn't included in toc
    document.body.insertBefore(panel, document.body.firstChild);

    // initial panel state
    if (startOpen === "true" || (startOpen === "auto" && !isSmallScreen()))
      openPanel();
    else closePanel();

    // attach click, scroll, and hash change listeners to window
    window.addEventListener("click", onClick);
    window.addEventListener("scroll", onScroll);
    window.addEventListener("hashchange", onScroll);
    window.addEventListener("keyup", onKeyUp);
    onScroll();

    // add class to push document body down out of way of toc button
    document.body.classList.add("toc_body_nudge");
  }

  // determine if screen wide enough to fit toc panel
  function isSmallScreen() {
    // in default theme:
    // 816px = 8.5in = width of "page" (<body>) element
    // 260px = min width of toc panel (*2 for both sides of <body>)
    return window.innerWidth < 816 + 260 * 2;
  }

  // when mouse is clicked anywhere in window
  function onClick() {
    if (isSmallScreen()) closePanel();
  }

  // when window is scrolled or hash changed
  function onScroll() {
    highlightViewed();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    // close on esc
    if (event.key === "Escape") closePanel();
  }

  // find entry of currently viewed document section in toc and highlight
  function highlightViewed() {
    const firstId = getFirstInView(typesQuery);

    // get toc entries (links), unhighlight all, then highlight viewed
    const list = document.getElementById("toc_list");
    if (!firstId || !list) return;
    const links = list.querySelectorAll("a");
    for (const link of links) link.dataset.viewing = "false";
    const link = list.querySelector('a[href="#' + firstId + '"]');
    if (!link) return;
    link.dataset.viewing = "true";
  }

  // get first or previous toc listed element in top half of view
  function getFirstInView(query) {
    // get all elements matching query and with id
    const elements = document.querySelectorAll(query);
    const elementsWithIds = [];
    for (const element of elements) {
      if (element.id) elementsWithIds.push(element);
    }

    // get first or previous element in top half of view
    for (let i = 0; i < elementsWithIds.length; i++) {
      const element = elementsWithIds[i];
      const prevElement = elementsWithIds[Math.max(0, i - 1)];
      if (element.getBoundingClientRect().top >= 0) {
        if (element.getBoundingClientRect().top < window.innerHeight / 2)
          return element.id;
        else return prevElement.id;
      }
    }
  }

  // make panel
  function makePanel() {
    // create panel
    const panel = document.createElement("div");
    panel.id = "toc_panel";
    if (bullets === "true") panel.dataset.bullets = "true";

    // create header
    const header = document.createElement("div");
    header.id = "toc_header";

    // create toc button
    const button = document.createElement("button");
    button.id = "toc_button";
    button.innerHTML = document.querySelector(".icon_th_list").innerHTML;
    button.title = "Table of Contents";
    button.classList.add("icon_button");

    // create header text
    const text = document.createElement("h4");
    text.innerHTML = "Table of Contents";

    // create container for toc list
    const list = document.createElement("div");
    list.id = "toc_list";

    // attach click listeners
    panel.addEventListener("click", onPanelClick);
    header.addEventListener("click", onHeaderClick);
    button.addEventListener("click", onButtonClick);

    // attach elements
    header.appendChild(button);
    header.appendChild(text);
    panel.appendChild(header);
    panel.appendChild(list);

    return panel;
  }

  // create toc entries (links) to each element of the specified types
  function makeEntries(panel) {
    const elements = document.querySelectorAll(typesQuery);
    for (const element of elements) {
      // do not add link if element doesn't have assigned id
      if (!element.id) continue;

      // create link/list item
      const link = document.createElement("a");
      link.classList.add("toc_link");
      switch (element.tagName.toLowerCase()) {
        case "h1":
          link.dataset.level = "1";
          break;
        case "h2":
          link.dataset.level = "2";
          break;
        case "h3":
          link.dataset.level = "3";
          break;
        case "h4":
          link.dataset.level = "4";
          break;
      }
      link.title = element.innerText;
      let text = element.innerText;
      if (text.length > charLimit) text = text.slice(0, charLimit) + "...";
      link.innerHTML = text;
      link.href = "#" + element.id;
      link.addEventListener("click", onLinkClick);

      // attach link
      panel.querySelector("#toc_list").appendChild(link);
    }
  }

  // when panel is clicked
  function onPanelClick(event) {
    // stop click from propagating to window/document and closing panel
    event.stopPropagation();
  }

  // when header itself is clicked
  function onHeaderClick(event) {
    togglePanel();
  }

  // when button is clicked
  function onButtonClick(event) {
    togglePanel();
    // stop header underneath button from also being clicked
    event.stopPropagation();
  }

  // when link is clicked
  function onLinkClick(event) {
    if (clickClose === "true" || (clickClose === "auto" && isSmallScreen()))
      closePanel();
    else openPanel();
  }

  // open panel if closed, close if opened
  function togglePanel() {
    const panel = document.getElementById("toc_panel");
    if (!panel) return;

    if (panel.dataset.open === "true") closePanel();
    else openPanel();
  }

  // open panel
  function openPanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "true";
  }

  // close panel
  function closePanel() {
    const panel = document.getElementById("toc_panel");
    if (panel) panel.dataset.open = "false";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- th list icon -->

<template class="icon_th_list">
  <!-- modified from: https://fontawesome.com/icons/th-list -->
  <svg width="16" height="16" viewBox="0 0 512 512" tabindex="-1">
    <path
      fill="currentColor"
      d="M96 96c0 26.51-21.49 48-48 48S0 122.51 0 96s21.49-48 48-48 48 21.49 48 48zM48 208c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm0 160c-26.51 0-48 21.49-48 48s21.49 48 48 48 48-21.49 48-48-21.49-48-48-48zm96-236h352c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h352c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H144c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* toc panel */
    #toc_panel {
      box-sizing: border-box;
      position: fixed;
      top: 0;
      left: 0;
      background: #ffffff;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      z-index: 2;
    }

    /* toc panel when closed */
    #toc_panel[data-open="false"] {
      min-width: 60px;
      width: 60px;
      height: 60px;
      border-right: solid 1px #bdbdbd;
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc panel when open */
    #toc_panel[data-open="true"] {
      min-width: 260px;
      max-width: 480px;
      /* keep panel edge consistent distance away from "page" edge */
      width: calc(((100vw - 8.5in) / 2) - 30px - 40px);
      bottom: 0;
      border-right: solid 1px #bdbdbd;
    }

    /* toc panel header */
    #toc_header {
      box-sizing: border-box;
      display: flex;
      flex-direction: row;
      align-items: center;
      height: 60px;
      margin: 0;
      padding: 20px;
    }

    /* toc panel header when hovered */
    #toc_header:hover {
      cursor: pointer;
    }

    /* toc panel header when panel open */
    #toc_panel[data-open="true"] > #toc_header {
      border-bottom: solid 1px #bdbdbd;
    }

    /* toc open/close header button */
    #toc_button {
      margin-right: 20px;
    }

    /* hide toc list and header text when closed */
    #toc_panel[data-open="false"] > #toc_header > *:not(#toc_button),
    #toc_panel[data-open="false"] > #toc_list {
      display: none;
    }

    /* toc list of entries */
    #toc_list {
      box-sizing: border-box;
      width: 100%;
      padding: 20px;
      position: absolute;
      top: calc(60px + 1px);
      bottom: 0;
      overflow: auto;
    }

    /* toc entry, link to section in document */
    .toc_link {
      display: block;
      padding: 5px;
      position: relative;
      font-weight: 600;
      text-decoration: none;
    }

    /* toc entry when hovered or when "viewed" */
    .toc_link:hover,
    .toc_link[data-viewing="true"] {
      background: #f5f5f5;
    }

    /* toc entry, level 1 indentation */
    .toc_link[data-level="1"] {
      margin-left: 0;
    }

    /* toc entry, level 2 indentation */
    .toc_link[data-level="2"] {
      margin-left: 20px;
    }

    /* toc entry, level 3 indentation */
    .toc_link[data-level="3"] {
      margin-left: 40px;
    }

    /* toc entry, level 4 indentation */
    .toc_link[data-level="4"] {
      margin-left: 60px;
    }

    /* toc entry bullets */
    #toc_panel[data-bullets="true"] .toc_link[data-level]:before {
      position: absolute;
      left: -15px;
      top: -1px;
      font-size: 1.5em;
    }

    /* toc entry, level 2 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="2"]:before {
      content: "\2022";
    }

    /* toc entry, level 3 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="3"]:before {
      content: "\25AB";
    }

    /* toc entry, level 4 bullet */
    #toc_panel[data-bullets="true"] .toc_link[data-level="4"]:before {
      content: "-";
    }
  }

  /* when on screen < 8.5in wide */
  @media only screen and (max-width: 8.5in) {
    /* push <body> ("page") element down to make room for toc icon */
    .toc_body_nudge {
      padding-top: 60px;
    }

    /* toc icon when panel closed and not hovered */
    #toc_panel[data-open="false"]:not(:hover) {
      background: rgba(255, 255, 255, 0.75);
    }
  }

  /* always hide toc panel on print */
  @media only print {
    #toc_panel {
      display: none;
    }
  }
</style>
<!-- 
  Tooltips Plugin

  Makes it such that when the user hovers or focuses a link to a citation or
  figure, a tooltip appears with a preview of the reference content, along with
  arrows to navigate between instances of the same reference in the document.
-->

<script type="module">
  // whether user must click off to close tooltip instead of just un-hovering
  const clickClose = "false";
  // delay (in ms) between opening and closing tooltip
  const delay = "100";

  // start script
  function start() {
    const links = getLinks();
    for (const link of links) {
      // attach hover and focus listeners to link
      link.addEventListener("mouseover", onLinkHover);
      link.addEventListener("mouseleave", onLinkUnhover);
      link.addEventListener("focus", onLinkFocus);
      link.addEventListener("touchend", onLinkTouch);
    }

    // attach mouse, key, and resize listeners to window
    window.addEventListener("mousedown", onClick);
    window.addEventListener("touchstart", onClick);
    window.addEventListener("keyup", onKeyUp);
    window.addEventListener("resize", onResize);
  }

  // when link is hovered
  function onLinkHover() {
    // function to open tooltip
    const delayOpenTooltip = function () {
      openTooltip(this);
    }.bind(this);

    // run open function after delay
    this.openTooltipTimer = window.setTimeout(delayOpenTooltip, delay);
  }

  // when mouse leaves link
  function onLinkUnhover() {
    // cancel opening tooltip
    window.clearTimeout(this.openTooltipTimer);

    // don't close on unhover if option specifies
    if (clickClose === "true") return;

    // function to close tooltip
    const delayCloseTooltip = function () {
      // if tooltip open and if mouse isn't over tooltip, close
      const tooltip = document.getElementById("tooltip");
      if (tooltip && !tooltip.matches(":hover")) closeTooltip();
    };

    // run close function after delay
    this.closeTooltipTimer = window.setTimeout(delayCloseTooltip, delay);
  }

  // when link is focused (tabbed to)
  function onLinkFocus(event) {
    openTooltip(this);
  }

  // when link is touched on touch screen
  function onLinkTouch(event) {
    // attempt to force hover state on first tap always, and trigger
    // regular link click (and navigation) on second tap
    if (event.target === document.activeElement) event.target.click();
    else {
      document.activeElement.blur();
      event.target.focus();
    }
    if (event.cancelable) event.preventDefault();
    event.stopPropagation();
    return false;
  }

  // when mouse is clicked anywhere in window
  function onClick(event) {
    closeTooltip();
  }

  // when key pressed
  function onKeyUp(event) {
    if (!event || !event.key) return;

    switch (event.key) {
      // trigger click of prev button
      case "ArrowLeft":
        const prevButton = document.getElementById("tooltip_prev_button");
        if (prevButton) prevButton.click();
        break;
      // trigger click of next button
      case "ArrowRight":
        const nextButton = document.getElementById("tooltip_next_button");
        if (nextButton) nextButton.click();
        break;
      // close on esc
      case "Escape":
        closeTooltip();
        break;
    }
  }

  // when window is resized or zoomed
  function onResize() {
    closeTooltip();
  }

  // get all links of types we wish to handle
  function getLinks() {
    const queries = [];
    // exclude buttons, anchor links, toc links, etc
    const exclude =
      ":not(.button):not(.icon_button):not(.anchor):not(.toc_link)";
    queries.push('a[href^="#ref-"]' + exclude); // citation links
    queries.push('a[href^="#fig:"]' + exclude); // figure links
    const query = queries.join(", ");
    return document.querySelectorAll(query);
  }

  // get links with same target, get index of link in set, get total
  // same links
  function getSameLinks(link) {
    const sameLinks = [];
    const links = getLinks();
    for (const otherLink of links) {
      if (otherLink.getAttribute("href") === link.getAttribute("href"))
        sameLinks.push(otherLink);
    }

    return {
      elements: sameLinks,
      index: sameLinks.indexOf(link),
      total: sameLinks.length,
    };
  }

  // open tooltip
  function openTooltip(link) {
    // delete tooltip if it exists, start fresh
    closeTooltip();

    // make tooltip element
    const tooltip = makeTooltip(link);

    // if source couldn't be found and tooltip not made, exit
    if (!tooltip) return;

    // make navbar elements
    const navBar = makeNavBar(link);
    if (navBar) tooltip.firstElementChild.appendChild(navBar);

    // attach tooltip to page
    document.body.appendChild(tooltip);

    // position tooltip
    const position = function () {
      positionTooltip(link);
    };
    position();

    // if tooltip contains images, position again after they've loaded
    const imgs = tooltip.querySelectorAll("img");
    for (const img of imgs) img.addEventListener("load", position);
  }

  // close (delete) tooltip
  function closeTooltip() {
    const tooltip = document.getElementById("tooltip");
    if (tooltip) tooltip.remove();
  }

  // make tooltip
  function makeTooltip(link) {
    // get target element that link points to
    const source = getSource(link);

    // if source can't be found, exit
    if (!source) return;

    // create new tooltip
    const tooltip = document.createElement("div");
    tooltip.id = "tooltip";
    const tooltipContent = document.createElement("div");
    tooltipContent.id = "tooltip_content";
    tooltip.appendChild(tooltipContent);

    // make copy of source node and put in tooltip
    const sourceCopy = makeCopy(source);
    tooltipContent.appendChild(sourceCopy);

    // attach mouse event listeners
    tooltip.addEventListener("click", onTooltipClick);
    tooltip.addEventListener("mousedown", onTooltipClick);
    tooltip.addEventListener("touchstart", onTooltipClick);
    tooltip.addEventListener("mouseleave", onTooltipUnhover);

    // (for interaction with lightbox plugin)
    // transfer click on tooltip copied img to original img
    const sourceImg = source.querySelector("img");
    const sourceCopyImg = sourceCopy.querySelector("img");
    if (sourceImg && sourceCopyImg) {
      const clickImg = function () {
        sourceImg.click();
        closeTooltip();
      };
      sourceCopyImg.addEventListener("click", clickImg);
    }

    return tooltip;
  }

  // make carbon copy of html dom element
  function makeCopy(source) {
    const sourceCopy = source.cloneNode(true);

    // delete elements marked with ignore (eg anchor and jump buttons)
    const deleteFromCopy = sourceCopy.querySelectorAll('[data-ignore="true"]');
    for (const element of deleteFromCopy) element.remove();

    // delete certain element attributes
    const attributes = [
      "id",
      "data-collapsed",
      "data-selected",
      "data-highlighted",
      "data-glow",
      "class",
    ];
    for (const attribute of attributes) {
      sourceCopy.removeAttribute(attribute);
      const elements = sourceCopy.querySelectorAll("[" + attribute + "]");
      for (const element of elements) element.removeAttribute(attribute);
    }

    return sourceCopy;
  }

  // when tooltip is clicked
  function onTooltipClick(event) {
    // when user clicks on tooltip, stop click from transferring
    // outside of tooltip (eg, click off to close tooltip, or eg click
    // off to unhighlight same refs)
    event.stopPropagation();
  }

  // when tooltip is unhovered
  function onTooltipUnhover(event) {
    if (clickClose === "true") return;

    // make sure new mouse/touch/focus no longer over tooltip or any
    // element within it
    const tooltip = document.getElementById("tooltip");
    if (!tooltip) return;
    if (this.contains(event.relatedTarget)) return;

    closeTooltip();
  }

  // make nav bar to go betwen prev/next instances of same reference
  function makeNavBar(link) {
    // find other links to the same source
    const sameLinks = getSameLinks(link);

    // don't show nav bar when singular reference
    if (sameLinks.total <= 1) return;

    // find prev/next links with same target
    const prevLink = getPrevLink(link, sameLinks);
    const nextLink = getNextLink(link, sameLinks);

    // create nav bar
    const navBar = document.createElement("div");
    navBar.id = "tooltip_nav_bar";
    const text = sameLinks.index + 1 + " of " + sameLinks.total;

    // create nav bar prev/next buttons
    const prevButton = document.createElement("button");
    const nextButton = document.createElement("button");
    prevButton.id = "tooltip_prev_button";
    nextButton.id = "tooltip_next_button";
    prevButton.title =
      "Jump to the previous occurence of this item in the document [←]";
    nextButton.title =
      "Jump to the next occurence of this item in the document [→]";
    prevButton.classList.add("icon_button");
    nextButton.classList.add("icon_button");
    prevButton.innerHTML = document.querySelector(".icon_caret_left").innerHTML;
    nextButton.innerHTML =
      document.querySelector(".icon_caret_right").innerHTML;
    navBar.appendChild(prevButton);
    navBar.appendChild(document.createTextNode(text));
    navBar.appendChild(nextButton);

    // attach click listeners to buttons
    prevButton.addEventListener("click", function () {
      onPrevNextClick(link, prevLink);
    });
    nextButton.addEventListener("click", function () {
      onPrevNextClick(link, nextLink);
    });

    return navBar;
  }

  // get previous link with same target
  function getPrevLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if < 1
    let index;
    if (sameLinks.index - 1 >= 0) index = sameLinks.index - 1;
    else index = sameLinks.total - 1;
    return sameLinks.elements[index];
  }

  // get next link with same target
  function getNextLink(link, sameLinks) {
    if (!sameLinks) sameLinks = getSameLinks(link);
    // wrap index to other side if > total
    let index;
    if (sameLinks.index + 1 <= sameLinks.total - 1) index = sameLinks.index + 1;
    else index = 0;
    return sameLinks.elements[index];
  }

  // get element that is target of link or url hash
  function getSource(link) {
    const hash = link ? link.hash : window.location.hash;
    const id = hash.slice(1);
    let target = document.querySelector('[id="' + id + '"]');
    if (!target) return;

    // if ref or figure, modify target to get expected element
    if (id.indexOf("ref-") === 0) target = target.querySelector(":nth-child(2)");
    else if (id.indexOf("fig:") === 0) target = target.querySelector("figure");

    return target;
  }

  // when prev/next arrow button is clicked
  function onPrevNextClick(link, prevNextLink) {
    if (link && prevNextLink)
      goToElement(prevNextLink, window.innerHeight * 0.5);
  }

  // scroll to and focus element
  function goToElement(element, offset) {
    // expand accordion section if collapsed
    expandElement(element);
    const y =
      getRectInView(element).top -
      getRectInView(document.documentElement).top -
      (offset || 0);
    // trigger any function listening for "onscroll" event
    window.dispatchEvent(new Event("scroll"));
    window.scrollTo(0, y);
    document.activeElement.blur();
    element.focus();
  }

  // determine position to place tooltip based on link position in
  // viewport and tooltip size
  function positionTooltip(link, left, top) {
    const tooltipElement = document.getElementById("tooltip");
    if (!tooltipElement) return;

    // get convenient vars for position/dimensions of
    // link/tooltip/page/view
    link = getRectInPage(link);
    const tooltip = getRectInPage(tooltipElement);
    const view = getRectInPage();

    // horizontal positioning
    if (left)
      // use explicit value
      left = left;
    else if (link.left + tooltip.width < view.right)
      // fit tooltip to right of link
      left = link.left;
    else if (link.right - tooltip.width > view.left)
      // fit tooltip to left of link
      left = link.right - tooltip.width;
    // center tooltip in view
    else left = (view.right - view.left) / 2 - tooltip.width / 2;

    // vertical positioning
    if (top)
      // use explicit value
      top = top;
    else if (link.top - tooltip.height > view.top)
      // fit tooltip above link
      top = link.top - tooltip.height;
    else if (link.bottom + tooltip.height < view.bottom)
      // fit tooltip below link
      top = link.bottom;
    else {
      // center tooltip in view
      top = view.top + view.height / 2 - tooltip.height / 2;
      // nudge off of link to left/right if possible
      if (link.right + tooltip.width < view.right) left = link.right;
      else if (link.left - tooltip.width > view.left)
        left = link.left - tooltip.width;
    }

    tooltipElement.style.left = left + "px";
    tooltipElement.style.top = top + "px";
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- caret left icon -->

<template class="icon_caret_left">
  <!-- modified from: https://fontawesome.com/icons/caret-left -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M192 127.338v257.324c0 17.818-21.543 26.741-34.142 14.142L29.196 270.142c-7.81-7.81-7.81-20.474 0-28.284l128.662-128.662c12.599-12.6 34.142-3.676 34.142 14.142z"
    ></path>
  </svg>
</template>

<!-- caret right icon -->

<template class="icon_caret_right">
  <!-- modified from: https://fontawesome.com/icons/caret-right -->
  <svg width="16" height="16" viewBox="0 0 192 512">
    <path
      fill="currentColor"
      d="M0 384.662V127.338c0-17.818 21.543-26.741 34.142-14.142l128.662 128.662c7.81 7.81 7.81 20.474 0 28.284L34.142 398.804C21.543 411.404 0 402.48 0 384.662z"
    ></path>
  </svg>
</template>

<style>
  @media only screen {
    /* tooltip container */
    #tooltip {
      position: absolute;
      width: 50%;
      min-width: 240px;
      max-width: 75%;
      z-index: 1;
    }

    /* tooltip content */
    #tooltip_content {
      margin-bottom: 5px;
      padding: 20px;
      border-radius: 5px;
      border: solid 1px #bdbdbd;
      box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
      background: #ffffff;
      overflow-wrap: break-word;
    }

    /* tooltip copy of paragraphs and figures */
    #tooltip_content > p,
    #tooltip_content > figure {
      margin: 0;
      max-height: 320px;
      overflow-y: auto;
    }

    /* tooltip copy of <img> */
    #tooltip_content > figure > img,
    #tooltip_content > figure > svg {
      max-height: 260px;
    }

    /* navigation bar */
    #tooltip_nav_bar {
      margin-top: 10px;
      text-align: center;
    }

    /* navigation bar previous/next buton */
    #tooltip_nav_bar > .icon_button {
      position: relative;
      top: 3px;
    }

    /* navigation bar previous button */
    #tooltip_nav_bar > .icon_button:first-of-type {
      margin-right: 5px;
    }

    /* navigation bar next button */
    #tooltip_nav_bar > .icon_button:last-of-type {
      margin-left: 5px;
    }
  }

  /* always hide tooltip on print */
  @media only print {
    #tooltip {
      display: none;
    }
  }
</style>
<!--
  Analytics Plugin (third-party) 
  
  Copy and paste code from Google Analytics or similar service here.
-->
<!-- 
  Annotations Plugin

  Allows public annotation of the  manuscript. See https://web.hypothes.is/.
-->

<script type="module">
  // configuration
  window.hypothesisConfig = function () {
    return {
      branding: {
        accentColor: "#2196f3",
        appBackgroundColor: "#f8f8f8",
        ctaBackgroundColor: "#f8f8f8",
        ctaTextColor: "#000000",
        selectionFontFamily: "Open Sans, Helvetica, sans serif",
        annotationFontFamily: "Open Sans, Helvetica, sans serif",
      },
    };
  };

  // hypothesis client script
  const embed = "https://hypothes.is/embed.js";
  // hypothesis annotation count query url
  const query = "https://api.hypothes.is/api/search?limit=0&url=";

  // start script
  function start() {
    const button = makeButton();
    document.body.insertBefore(button, document.body.firstChild);
    insertCount(button);
  }

  // make button
  function makeButton() {
    // create button
    const button = document.createElement("button");
    button.id = "hypothesis_button";
    button.innerHTML = document.querySelector(".icon_hypothesis").innerHTML;
    button.title = "Hypothesis annotations";
    button.classList.add("icon_button");

    function onClick(event) {
      onButtonClick(event, button);
    }

    // attach click listeners
    button.addEventListener("click", onClick);

    return button;
  }

  // insert annotations count
  async function insertCount(button) {
    // get annotation count from Hypothesis based on url
    let count = "-";
    try {
      const canonical = document.querySelector('link[rel="canonical"]');
      const location = window.location;
      const url = encodeURIComponent((canonical || location).href);
      const response = await fetch(query + url);
      const json = await response.json();
      count = json.total || "-";
    } catch (error) {
      console.log(error);
    }

    // put count into button
    const counter = document.createElement("span");
    counter.id = "hypothesis_count";
    counter.innerHTML = count;
    button.title = "View " + count + " Hypothesis annotations";
    button.append(counter);
  }

  // when button is clicked
  function onButtonClick(event, button) {
    const script = document.createElement("script");
    script.src = embed;
    document.body.append(script);
    button.remove();
  }

  // start script when document is finished loading
  window.addEventListener("load", start);
</script>

<!-- hypothesis icon -->

<template class="icon_hypothesis">
  <!-- modified from: https://simpleicons.org/icons/hypothesis.svg / https://git.io/Jf1VB -->
  <svg width="16" height="16" viewBox="0 0 24 24" tabindex="-1">
    <path
      fill="currentColor"
      d="M3.43 0C2.5 0 1.72 .768 1.72 1.72V18.86C1.72 19.8 2.5 20.57 3.43 20.57H9.38L12 24L14.62 20.57H20.57C21.5 20.57 22.29 19.8 22.29 18.86V1.72C22.29 .77 21.5 0 20.57 0H3.43M5.14 3.43H7.72V9.43S8.58 7.72 10.28 7.72C12 7.72 13.74 8.57 13.74 11.24V17.14H11.16V12C11.16 10.61 10.28 10.07 9.43 10.29C8.57 10.5 7.72 11.41 7.72 13.29V17.14H5.14V3.43M18 13.72C18.95 13.72 19.72 14.5 19.72 15.42A1.71 1.71 0 0 1 18 17.13A1.71 1.71 0 0 1 16.29 15.42C16.29 14.5 17.05 13.71 18 13.71Z"
      tabindex="-1"
    ></path>
  </svg>
</template>

<style>
  /* hypothesis activation button */
  #hypothesis_button {
    box-sizing: border-box;
    position: fixed;
    top: 0;
    right: 0;
    width: 60px;
    height: 60px;
    background: #ffffff;
    border-radius: 0;
    border-left: solid 1px #bdbdbd;
    border-bottom: solid 1px #bdbdbd;
    box-shadow: 0 0 20px rgba(0, 0, 0, 0.05);
    z-index: 2;
  }

  /* hypothesis button svg */
  #hypothesis_button > svg {
    position: relative;
    top: -4px;
  }

  /* hypothesis annotation count */
  #hypothesis_count {
    position: absolute;
    left: 0;
    right: 0;
    bottom: 5px;
  }

  /* side panel */
  .annotator-frame {
    width: 280px !important;
  }

  /* match highlight color to rest of theme */
  .annotator-highlights-always-on .annotator-hl {
    background-color: #ffeb3b !important;
  }

  /* match focused color to rest of theme */
  .annotator-hl.annotator-hl-focused {
    background-color: #ff8a65 !important;
  }

  /* match bucket bar color to rest of theme */
  .annotator-bucket-bar {
    background: #f5f5f5 !important;
  }

  /* always hide button, toolbar, and tooltip on print */
  @media only print {
    #hypothesis_button {
      display: none;
    }

    .annotator-frame {
      display: none !important;
    }

    hypothesis-adder {
      display: none !important;
    }
  }
</style>
<!-- 
  Mathjax Plugin (third-party) 

  Allows the proper rendering of math/equations written in LaTeX.
  See https://www.mathjax.org/.
-->

<script type="text/x-mathjax-config">
  // configuration
  MathJax.Hub.Config({
    "CommonHTML": { linebreaks: { automatic: true } },
    "HTML-CSS": { linebreaks: { automatic: true } },
    "SVG": { linebreaks: { automatic: true } },
    "fast-preview": { disabled: true }
  });
</script>

<script
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
  integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A=="
  crossorigin="anonymous"
></script>

<style>
  /* mathjax containers */
  .math.display > span:not(.MathJax_Preview) {
    /* turn inline element (no dimensions) into block (allows fixed width and thus scrolling) */
    display: flex !important;
    overflow-x: auto !important;
    overflow-y: hidden !important;
    justify-content: center;
    align-items: center;
    margin: 0 !important;
  }

  /* right click menu */
  .MathJax_Menu {
    border-radius: 5px !important;
    border: solid 1px #bdbdbd !important;
    box-shadow: none !important;
  }

  /* equation auto-number */
  span[id^="eq:"] > span.math.display + span {
    font-weight: 600;
  }

  /* equation */
  span[id^="eq:"] > span.math.display > span {
    /* nudge to make room for equation auto-number and anchor */
    margin-right: 60px !important;
  }
</style>
</body>
</html>
