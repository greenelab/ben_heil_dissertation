## Background: Reproducibility

#### What is computational reproducibility

Reproducibility is a topic often discussed in scientific circles, and it means different things to different people [@doi:10.1190/1.1822162; @doi:10.1373/clinchem.2017.279984; @doi:10.1126/scitranslmed.aaf5027; @doi:10.1101/066803; @doi:10.3389/fninf.2017.00076; @doi:10.1109/MCSE.2009.15].
For the sake of clarity, we'll operate from Stodden et al's definition "the ability to recreate computational results from the data and code used by the original researcher" (http://stodden.net/icerm_report.pdf).
We would like to add one caveat though.
The language surrounding reproducibility is often binary, as in "reproducible research" or "an irreproducible paper". 
In reality, reproducibility falls on a sliding scale based on how long it takes a scientist to reproduce a work. 
For poorly specified work, it could take forever as the conditions that allowed the research will never happen again.
For extremely high quality research, it could take a scientist only seconds of their time to press the "run" button on the original authors' code and get the same results.

#### Why does it matter?
Now that we've defined what reproducibility is, we can discuss why it matters.
In her book "Why Trust Science?", Naomi Oreskes argues the answer to the eponymous question is that the process of coming to a consensus is what makes science trustworthy [@isbn:9780691179001].
In a world where all papers take forever to reproduce, it would be challenging to come to the consensus required to do trustworthy science.

Another way of viewing the scientific method is the Popperian idea of falsifiable theories [@isbn:0415278449].
Theories are constructed from evidence and from reproduction of the same findings about the world.
If a theory can't be reproduced, then it can't be supported or proven false, and isn't science under Popper's definition [@doi:10.1109/MS.2018.2883805].

Those points are fairly philisophical though.
If you're looking for a discussion of concrete impacts of failures in computational reproducibility, we recommend Ivie and Thain's review paper [@doi:10.1145/3186266].
They point out that in the biological domain preclinical drug development studies could be replicated in only 10-25% of cases [@doi:10.1038/483531a; @doi:10.1038/nrd3439-c1].
Similarly, only about 50% of ACM papers were able to be built from their source code [@doi:10.1145/2812803].
In general, lack of reproducibility could cause a lack of trust in science [@doi:10.1177/1745691612465253].

Reproducibility isn't all about preventing bad things from happening though.
Having code that is easy to run helps verify that code is bug-free, and makes it easier for the original author to run in the future.
It also allows remixing research code, leading to greater accessibility of scientific research.
Because the authors working on latent diffusion models for image synthesis made their code available, others quickly created an optimized version allowing those without a powerful GPU to run it [@arxiv:2112.10752; https://github.com/CompVis/stable-diffusion; https://github.com/basujindal/stable-diffusion/]

#### What can be done?

The question remains: what can be done to increase the reproducibility of scientific work?
Krafczyk et al. argue that the keys are to document well, write code for ease of executability, and make code deterministic.
Alternatively, we could create a central repository of data and code used in research like we have a repository for articles in PubMed Central [@doi:10.1126/science.1213847].
In fact, the field of machine learning has something similar in Papers with Code (https://paperswithcode.com/), a website where you can browse only the machine learning preprints and papers that have associated code.
The epitome of reproducibility is probably something like executable papers a la Distill (https://distill.pub/) or eLife's Executable Research Articles [@tsang2020].
In chapter 2, we discuss options to make machine learning research in the life sciences reproducible in more depth, and give our own recommendations.
