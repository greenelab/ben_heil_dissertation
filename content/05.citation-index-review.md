
## Citation indices

Over the past century quantifying the progress of science has become popular.
Even before computers made it easy to collate information about publications, work had already begun to evaluate papers based on their number of citations [@doi:10.1126/science.122.3159.108].
There is even a book about it [@isbn:1108492665].

Determining the relative "impact" of different authors and journals is a perennial question when measuring science.
One of the most commonly used metrics in this space is the h-index, which balances an author's number of publications with the number of citations each receives [@doi:10.1073/pnas.0507655102]. 
However, the h-index is not a perfect metric [@doi:10.1016/j.acalib.2017.08.013] and has arguably become less useful in recent years [@doi:10.1371/journal.pone.0253397].
Other metrics, like the g-index[@doi:10.1007/s11192-006-0144-7] and the i-10 index (https://scholar.google.com/), try to improve on the h-index by placing a higher weight on more highly cited papers.

There are metrics for comparing journals as well.
The Journal Impact Factor [@jif] is the progenitor journal metric, evaluating journals based on how many citations the average paper in that journal has received over the past few years.
Other measures use a more network-based approach to quantifying journals' importance.
The most common are Eigenfactor [@eigenfactor] and the SCImago Journal Rank (https://www.scimagojr.com/), which use variations on the PageRank algorithm to evaluate the importance of various journals. 

Academic articles are arguably the main building blocks of scientific communication, so it makes sense to try to understand which ones are the most important.
Citation count seems like an obvious choice, but differences in citation practices between fields [@doi:10.1016/j.joi.2013.08.002] make it too crude a measure of impact.
Instead, many other metrics have been developed to choose which papers to read.

Many of these methods work by analyzing the graph formed by treating articles as nodes and citations as edges.
PageRank[@pagerank], one of the most influential methods for ranking nodes' importance in a graph, can also be applied to ranking papers [@doi:10.1073/pnas.0706851105].
It is not the only graph-based method, though.
Other centrality calculation methods, such as betweenness centrality, would make sense to use but are prohibitively computationally expensive to run.
Instead, methods like the disruption index [@doi:10.1038/s41586-019-0941-9] and its variants [@doi:10.1162/qss_a_00068] are more often used.

Some lines of research try to quantify other desirable characteristics of papers.
For example, Foster et al. claim to measure innovation by looking at papers that create new connections between known chemical entities [@doi:10.1177/0003122415601618].
Likewise, Wang et al. define novel papers as those that cite papers from unusual combinations of journals [@doi:10.1016/j.respol.2017.06.006].
The Altmetric Attention Score (https://www.altmetric.com/) goes even further, measuring the attention on a paper from outside the standard academic channels.

These metrics do not stand alone, however.
Much work has gone into improving the various methods by shoring up their weaknesses or normalizing them to make them more comparable across fields.
The relative citation ratio makes citation counts comparable across fields by normalizing it according to other papers in its neighborhood of the citation network [@doi:10.1371/journal.pbio.1002541].
Similarly, the source-normalized impact per paper normalizes article citation counts based on the total number of citations in the whole field [@doi:10.1016/j.joi.2010.01.002].
Several methods modify PageRank, such as Topical PageRank, which incorporates topic and journal prestige information into the PageRank calculation [@doi:10.1007/s11192-017-2626-1], and 
Vaccario et al.'s page and field rescaled PageRank, which accounts for differences between papers' ages and fields [@arxiv:1703.08071].
There are also several variants of the disruption index [@doi:10.1162/qss_a_00068].

Of course, these methods only work with data to train and evaluate them on.
We have come a long way from Garfield's "not unreasonable" proposal to aggregate one million citations manually [@doi:10.1126/science.122.3159.108].
These days we have several datasets with hundreds of millions to billions of references (https://www.webofknowledge.com, https://www.scopus.com  @doi:10.1007/s11192-019-03217-6).

Quantifying science could be better, however.
In addition to the shortcomings of individual methods [@doi:10.1523/JNEUROSCI.0002-08.2008; @doi:10.1016/j.wneu.2012.01.052; @doi:10.2106/00004623-200312000-00028], there are issues inherent to reducing the process of science to numbers.
To quote Alfred Korzybski, "the map is not the territory." 
Metrics of science truly measure quantitative relationships like mean citation counts, despite purporting to reflect "impact," "disruption," or "novelty."
If we forget that, we can mistake useful tools for arbiters of ground truth.

In chapter 5, we dive into one such shortcoming by demonstrating differences in article PageRanks between fields.
There we argue that normalizing out field-specific differences obscures useful signal and propose new directions of research for future citation metrics.
