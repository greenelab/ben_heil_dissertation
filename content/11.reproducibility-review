## Reproducibility

Reproducibility is a topic often discussed in scientific circles and means different things to different people [@doi:10.1190/1.1822162; @doi:10.1373/clinchem.2017.279984; @doi:10.1126/scitranslmed.aaf5027; @doi:10.1101/066803; @doi:10.3389/fninf.2017.00076; @doi:10.1109/MCSE.2009.15].
For clarity, we will operate from Stodden et al.'s definition of "the ability to recreate computational results from the data and code used by the original researcher" (http://stodden.net/icerm_report.pdf).
We would like to add one caveat, though.
The language surrounding reproducibility is often binary, as in "reproducible research" or "an irreproducible paper." 
In reality, reproducibility falls on a sliding scale based on how long it takes a scientist to reproduce a work. 
For poorly specified work, it could take forever as the conditions that allowed the research would never happen again.
For extremely high-quality research, it could take a scientist only seconds of their time to press the "run" button on the original authors' code and get the same results.

### Why does reproducibility matter?
Now that we have defined what reproducibility is, we can discuss why it matters.
In her book "Why Trust Science?" Naomi Oreskes argues that the answer to the eponymous question is that the process of coming to a consensus is what makes science trustworthy [@isbn:9780691179001].
In a world where all papers take forever to reproduce, reaching the consensus required to do trustworthy science would be challenging.

Another way of viewing the scientific method is the Popperian idea of falsifiable theories [@isbn:0415278449].
Theories are constructed from evidence and reproduction of the same findings about the world.
If a theory cannot be reproduced, then it can not be supported or proven false, and it is not science under Popper's definition [@doi:10.1109/MS.2018.2883805].

Those points are somewhat philosophical, though.
For a discussion of concrete impacts of failures in computational reproducibility, we recommend Ivie and Thain's review paper [@doi:10.1145/3186266].
They point out that preclinical drug development studies could be replicated in only 10-25% of cases [@doi:10.1038/483531a; @doi:10.1038/nrd3439-c1].
Similarly, only about 50% of ACM papers could be built from their source code [@doi:10.1145/2812803].
In general, a lack of reproducibility could cause a lack of trust in science [@doi:10.1177/1745691612465253].

Reproducibility is not all about more than preventing bad things from happening.
Having code that is easy to run helps verify that code is bug-free and makes it easier for the original author to run in the future.
It also allows remixing research code, leading to greater accessibility of scientific research.
Because the authors working on latent diffusion models for image synthesis made their code available [@arxiv:2112.10752], others quickly created an optimized version allowing those without a powerful GPU to run it (https://github.com/CompVis/stable-diffusion, https://github.com/basujindal/stable-diffusion/)

### What can be done?

The question remains: what can increase the reproducibility of scientific work?
Krafczyk et al. argue that the keys are to document well, write code for ease of executability, and make code deterministic.
Alternatively, we could create a central repository of data and code used in research similar to the repository for articles in PubMed Central [@doi:10.1126/science.1213847].
The field of machine learning has something similar in Papers with Code (https://paperswithcode.com/), a website where you can browse only the machine learning preprints and papers that have associated code.
The epitome of reproducibility is probably something like executable papers a la Distill (https://distill.pub/) or eLife's Executable Research Articles [@tsang2020].
In chapter 2, we discuss options to make machine learning research in the life sciences reproducible in more depth and give our own recommendations.
