
- Prioritizing papers is important. Lots of work has been done to rank papers/journals/etc.

Even before computers made it easy to collate information about publications, work had already begun to evaluate papers based on their number of citations [@doi:10.1126/science.122.3159.108].

Citation count seems like an obvious choice, but it's flawed because of e.g. differences in citation rates between fields.
Instead, many other metrics have been developed to choose which papers to read.

- Journal/author based
h-index, eigenfactor, JIF, Scimago Journal Rank (SJR)

- Graph-based
Pagerank, disruption index, 
Many datasets have been created to make this possible(e.g. WOS, COCI, etc)

- Metrics 
Some lines of research try to quanitfy other desirable characteristic of papers.
For example, Foster et al. claim to measure innovation by looking at papers that create new connections between known chemical entites [@doi:10.1177/0003122415601618].


- Lots of work has been done to normalize such metrics
RCR, source-normalized impact per paper (SNIP), topical PageRank, scaling network for pagerank a la Quantifying and suppressing ranking bias in a large citation network, 

- Text based/search methods
arxiv-sanity, elicit, LDA, tf-IDF, Science Concierge

- In this chapter we do X
